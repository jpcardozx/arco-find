#!/usr/bin/env python3
"""
üîß ARCO-FIND CONNECTION VALIDATOR
Valida conex√µes com BigQuery e SearchAPI e cria pipeline claro
"""

import asyncio
import sys
import os
from pathlib import Path
from datetime import datetime

# Add src to path
project_root = Path(__file__).parent.parent  # Go up one level from scripts/
src_path = project_root / "src"
sys.path.insert(0, str(src_path))
sys.path.insert(0, str(project_root))

async def validate_connections():
    """Valida todas as conex√µes API"""
    
    print("üîß VALIDANDO CONEX√ïES ARCO-FIND")
    print("=" * 50)
    
    results = {
        'bigquery': {'status': 'UNKNOWN', 'details': {}},
        'searchapi': {'status': 'UNKNOWN', 'details': {}},
        'connections_healthy': False
    }
    
    # 1. Validar BigQuery
    print("\nüìä VALIDANDO BIGQUERY...")
    try:
        from config.api_keys import APIConfig
        from google.cloud import bigquery
        
        api_config = APIConfig()
        
        # Tentar conectar ao BigQuery
        try:
            bq_client = bigquery.Client(project=api_config.GOOGLE_CLOUD_PROJECT)
            
            # Teste de query simples
            test_query = "SELECT 'BigQuery Connected' as status"
            query_job = bq_client.query(test_query)
            result = list(query_job.result())
            
            if result:
                results['bigquery']['status'] = 'CONNECTED'
                results['bigquery']['details'] = {
                    'project': api_config.GOOGLE_CLOUD_PROJECT,
                    'dataset': api_config.BIGQUERY_DATASET_ID,
                    'test_query_success': True
                }
                print(f"  ‚úÖ BigQuery CONECTADO - Projeto: {api_config.GOOGLE_CLOUD_PROJECT}")
            else:
                results['bigquery']['status'] = 'ERROR'
                print("  ‚ùå BigQuery - Query test failed")
                
        except Exception as e:
            results['bigquery']['status'] = 'ERROR'
            results['bigquery']['details'] = {'error': str(e)}
            print(f"  ‚ùå BigQuery ERRO: {e}")
            
    except ImportError as e:
        results['bigquery']['status'] = 'MISSING_DEPS'
        print(f"  ‚ö†Ô∏è BigQuery dependencies missing: {e}")
    
    # 2. Validar SearchAPI
    print("\nüîç VALIDANDO SEARCHAPI...")
    try:
        from src.connectors.searchapi_connector import SearchAPIConnector
        from config.api_keys import APIConfig
        
        api_config = APIConfig()
        search_api = SearchAPIConnector(api_key=api_config.SEARCH_API_KEY)
        
        # Teste de busca simples
        test_results = await search_api.search_companies("fintech", max_results=1)
        
        if test_results:
            results['searchapi']['status'] = 'CONNECTED'
            results['searchapi']['details'] = {
                'api_key_configured': bool(api_config.SEARCH_API_KEY),
                'test_search_success': True,
                'results_count': len(test_results)
            }
            print(f"  ‚úÖ SearchAPI CONECTADO - {len(test_results)} results")
        else:
            results['searchapi']['status'] = 'NO_RESULTS'
            print("  ‚ö†Ô∏è SearchAPI conectado mas sem resultados de teste")
            
    except Exception as e:
        results['searchapi']['status'] = 'ERROR'
        results['searchapi']['details'] = {'error': str(e)}
        print(f"  ‚ùå SearchAPI ERRO: {e}")
    
    # 3. Status geral
    bq_ok = results['bigquery']['status'] == 'CONNECTED'
    search_ok = results['searchapi']['status'] in ['CONNECTED', 'NO_RESULTS']
    
    results['connections_healthy'] = bq_ok and search_ok
    
    print(f"\nüéØ STATUS GERAL: {'‚úÖ SAUD√ÅVEL' if results['connections_healthy'] else '‚ö†Ô∏è NECESSITA ATEN√á√ÉO'}")
    
    return results

async def test_lead_discovery_pipeline():
    """Testa o pipeline de descoberta de leads"""
    
    print("\nüöÄ TESTANDO PIPELINE DE DESCOBERTA")
    print("=" * 50)
    
    try:
        from src.core.lead_qualification_engine import LeadQualificationEngine
        
        # Inicializar engine
        lead_engine = LeadQualificationEngine()
        print("  üìã LeadQualificationEngine inicializado")
        
        # Executar descoberta de leads
        print("  üîç Executando descoberta de leads...")
        start_time = datetime.now()
        
        qualified_leads = await lead_engine.discover_qualified_leads(target_count=3)
        
        execution_time = (datetime.now() - start_time).total_seconds()
        
        print(f"  ‚úÖ Descoberta conclu√≠da em {execution_time:.2f}s")
        print(f"  üéØ Leads qualificados encontrados: {len(qualified_leads)}")
        
        if qualified_leads:
            print("\n  üìä SAMPLE LEADS:")
            for i, lead in enumerate(qualified_leads[:2], 1):
                print(f"    {i}. {lead.company_name}")
                print(f"       Score: {lead.qualification_score}/100")
                print(f"       Priority: {lead.conversion_priority}")
        
        return {
            'pipeline_working': True,
            'execution_time': execution_time,
            'leads_found': len(qualified_leads),
            'sample_leads': qualified_leads[:2] if qualified_leads else []
        }
        
    except Exception as e:
        print(f"  ‚ùå Pipeline ERROR: {e}")
        return {
            'pipeline_working': False,
            'error': str(e)
        }

def create_pipeline_documentation():
    """Cria documenta√ß√£o clara do pipeline"""
    
    doc_content = '''# üéØ ARCO-FIND LEAD DISCOVERY PIPELINE

## üìä ARQUITETURA ATUAL

### 1. Core Components
- **LeadQualificationEngine**: Engine principal de qualifica√ß√£o
- **StrategicLeadOrchestrator**: Orquestrador de descoberta inteligente
- **SearchAPIConnector**: Integra√ß√£o com Meta Ads Library
- **BigQueryIntelligence**: Analytics e armazenamento

### 2. Data Sources
- **SearchAPI**: Meta Ads Library (dados reais de an√∫ncios)
- **BigQuery**: Google Cloud storage e analytics
- **PageSpeed API**: Performance analysis

### 3. Pipeline Flow
```
Input (Target Count) 
    ‚Üì
LeadQualificationEngine.discover_qualified_leads()
    ‚Üì
1. Check existing hot leads (BigQuery - FREE)
    ‚Üì
2. If needed, discover new leads (SearchAPI)
    ‚Üì
3. Qualify and score leads
    ‚Üì
4. Store in BigQuery for future use
    ‚Üì
Output (Qualified Leads List)
```

## üîß CURRENT STATUS

### ‚úÖ Working Components
- ‚úÖ API configurations loaded
- ‚úÖ SearchAPI connector functional
- ‚úÖ Lead qualification engine operational
- ‚úÖ Basic BigQuery integration

### ‚ö†Ô∏è Needs Improvement
- ‚ö†Ô∏è BigQuery schema validation
- ‚ö†Ô∏è Missing discover_strategic_prospects method
- ‚ö†Ô∏è Enhanced error handling
- ‚ö†Ô∏è Performance optimization

## üöÄ NEXT ACTIONS

### 1. Immediate Fixes
- [ ] Implement discover_strategic_prospects in SearchAPIConnector
- [ ] Validate/create BigQuery tables
- [ ] Fix qualification_score field issues

### 2. Pipeline Enhancement
- [ ] Add intelligent caching
- [ ] Implement rate limiting
- [ ] Add cost monitoring
- [ ] Create comprehensive logging

### 3. Documentation
- [ ] API usage guidelines
- [ ] Cost optimization strategies
- [ ] Error handling procedures

## üìà SUCCESS METRICS
- **Response Time**: < 2 seconds per lead discovery
- **API Cost**: < $1 per 100 qualified leads
- **Success Rate**: > 90% successful qualifications
- **Data Quality**: > 80% qualification score accuracy

## üéØ ULTIMATE GOAL
Automated, cost-efficient lead discovery pipeline that:
1. Identifies high-value prospects using real Meta Ads data
2. Qualifies leads with actionable intelligence
3. Stores results for strategic analysis
4. Scales efficiently with controlled costs
'''

    with open(project_root / "PIPELINE_DOCUMENTATION.md", "w", encoding="utf-8") as f:
        f.write(doc_content)
    
    print("üìÑ Pipeline documentation created: PIPELINE_DOCUMENTATION.md")

async def main():
    """Main validation and testing function"""
    
    print("üéØ ARCO-FIND CONNECTION & PIPELINE VALIDATOR")
    print("=" * 60)
    print(f"üìÖ Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. Validate connections
    connection_results = await validate_connections()
    
    # 2. Test pipeline
    pipeline_results = await test_lead_discovery_pipeline()
    
    # 3. Create documentation
    create_pipeline_documentation()
    
    # 4. Summary report
    print("\n" + "=" * 60)
    print("üìã VALIDATION SUMMARY")
    print("=" * 60)
    
    print(f"üîó BigQuery: {connection_results['bigquery']['status']}")
    print(f"üîç SearchAPI: {connection_results['searchapi']['status']}")
    print(f"üöÄ Pipeline: {'‚úÖ WORKING' if pipeline_results['pipeline_working'] else '‚ùå BROKEN'}")
    
    if pipeline_results['pipeline_working']:
        print(f"‚è±Ô∏è Execution Time: {pipeline_results['execution_time']:.2f}s")
        print(f"üéØ Leads Found: {pipeline_results['leads_found']}")
    
    print(f"\nüéØ Overall Status: {'‚úÖ OPERATIONAL' if connection_results['connections_healthy'] and pipeline_results['pipeline_working'] else '‚ö†Ô∏è NEEDS ATTENTION'}")
    
    # Save results
    import json
    results_file = project_root / f"validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'validation_timestamp': datetime.now().isoformat(),
            'connections': connection_results,
            'pipeline': pipeline_results
        }, f, indent=2, default=str)
    
    print(f"üíæ Results saved: {results_file.name}")

if __name__ == "__main__":
    asyncio.run(main())
