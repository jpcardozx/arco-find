#!/usr/bin/env python3
"""
üéØ FINANCIAL SIGNAL ORCHESTRATOR
Core module for revenue leak detection with freemium API integrations

ARCHITECTURE: Financial Signal Cascade with early elimination
- Wappalyzer SaaS stack detection ‚Üí vendor cost mapping
- Shopify Storefront subscription revenue analysis
- HTTP Archive performance-revenue correlation
- Meta Ad Library active spend detection
- RDAP domain authority validation
- Instagram traction verification

TARGET: <0.35s per domain, 85%+ early elimination, 75%+ leak score accuracy
"""

import os
import json
import asyncio
import aiohttp
import subprocess
import time
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import yaml
from google.cloud import bigquery
import numpy as np
from dotenv import load_dotenv

load_dotenv()

@dataclass
class FinancialSignals:
    """Financial intelligence signals for revenue leak detection"""
    domain: str
    
    # SaaS Stack Intelligence (40% weight)
    saas_tools: List[str]
    saas_monthly_cost: int
    saas_optimization_potential: int
    
    # Subscription Revenue Intelligence (25% weight) 
    subscription_revenue: int
    subscription_type: str
    subscription_platform: str
    
    # Performance Revenue Impact (20% weight)
    performance_loss: int
    performance_score: int
    js_bytes: int
    total_requests: int
    
    # Ad Spend Intelligence (10% weight)
    active_ads: bool
    estimated_ad_spend: int
    ad_platforms: List[str]
    
    # Domain Authority (5% weight)
    domain_age_months: int
    authority_score: int
    follower_count: int
    
    # Calculated Metrics
    leak_score: int
    priority: str
    total_potential_savings: int
    processing_time: float

class FinancialSignalOrchestrator:
    """
    Production-ready financial intelligence system
    
    Processes domains through cascade of financial signal detection
    with early elimination and freemium API optimization
    """
    
    def __init__(self):
        # Load vendor cost intelligence
        self.vendor_costs = self._load_vendor_costs()
        
        # API configurations
        self.meta_token = os.getenv('META_ACCESS_TOKEN')
        self.bigquery_client = bigquery.Client() if os.getenv('GOOGLE_APPLICATION_CREDENTIALS') else None
        
        # Processing statistics
        self.stats = {
            'total_processed': 0,
            'stage_1_eliminated': 0,  # SaaS cost < $80
            'stage_2_eliminated': 0,  # Subscription < $80
            'stage_3_eliminated': 0,  # Performance loss < $50
            'pipeline_qualified': 0,
            'average_processing_time': 0,
            'api_errors': 0
        }
        
        # Rate limiting
        self.api_calls = {
            'meta_ads': {'count': 0, 'reset_time': datetime.now() + timedelta(hours=1)},
            'instagram': {'count': 0, 'reset_time': datetime.now() + timedelta(hours=1)},
            'bigquery': {'bytes_processed': 0, 'reset_time': datetime.now() + timedelta(days=1)}
        }
        
        print("üéØ FINANCIAL SIGNAL ORCHESTRATOR")
        print("=" * 50)
        print(f"üí∞ Vendor costs loaded: {len(self.vendor_costs)}")
        print(f"üìä BigQuery client: {'‚úÖ' if self.bigquery_client else '‚ùå'}")
        print(f"üìà Meta API token: {'‚úÖ' if self.meta_token else '‚ùå'}")
        print("üöÄ Ready for financial intelligence processing")

    def _load_vendor_costs(self) -> Dict:
        """Load vendor cost intelligence from YAML"""
        
        vendor_costs = {
            # E-commerce Platforms
            'shopify_plus': {'monthly_cost': 299, 'optimization_potential': 0.3, 'category': 'platform'},
            'shopify': {'monthly_cost': 79, 'optimization_potential': 0.4, 'category': 'platform'},
            'woocommerce': {'monthly_cost': 25, 'optimization_potential': 0.5, 'category': 'platform'},
            'magento_commerce': {'monthly_cost': 1800, 'optimization_potential': 0.25, 'category': 'platform'},
            'vtex': {'monthly_cost': 500, 'optimization_potential': 0.3, 'category': 'platform'},
            
            # Email Marketing
            'klaviyo': {'monthly_cost': 150, 'optimization_potential': 0.4, 'category': 'email'},
            'mailchimp': {'monthly_cost': 100, 'optimization_potential': 0.5, 'category': 'email'},
            'constant_contact': {'monthly_cost': 80, 'optimization_potential': 0.6, 'category': 'email'},
            
            # Subscription Management
            'recharge': {'monthly_cost': 60, 'optimization_potential': 0.5, 'category': 'subscription'},
            'bold_subscriptions': {'monthly_cost': 80, 'optimization_potential': 0.4, 'category': 'subscription'},
            'chargebee': {'monthly_cost': 120, 'optimization_potential': 0.3, 'category': 'subscription'},
            
            # Customer Support
            'zendesk': {'monthly_cost': 120, 'optimization_potential': 0.4, 'category': 'support'},
            'intercom': {'monthly_cost': 200, 'optimization_potential': 0.3, 'category': 'support'},
            'freshdesk': {'monthly_cost': 60, 'optimization_potential': 0.5, 'category': 'support'},
            
            # Analytics & Forms
            'google_analytics_360': {'monthly_cost': 1250, 'optimization_potential': 0.6, 'category': 'analytics'},
            'typeform': {'monthly_cost': 50, 'optimization_potential': 0.6, 'category': 'forms'},
            'hubspot': {'monthly_cost': 200, 'optimization_potential': 0.3, 'category': 'crm'},
            
            # Marketing Automation
            'marketo': {'monthly_cost': 800, 'optimization_potential': 0.3, 'category': 'automation'},
            'pardot': {'monthly_cost': 400, 'optimization_potential': 0.4, 'category': 'automation'},
            'activecampaign': {'monthly_cost': 150, 'optimization_potential': 0.4, 'category': 'automation'}
        }
        
        return vendor_costs

    async def process_domain_financial_cascade(self, domain: str) -> Optional[FinancialSignals]:
        """
        Process domain through financial signal cascade
        
        Returns FinancialSignals if leak_score >= 75, None otherwise
        """
        
        start_time = time.time()
        self.stats['total_processed'] += 1
        
        try:
            # Stage 1: SaaS Stack Detection (Foundation - 40% weight)
            print(f"\nüîç Processing: {domain}")
            saas_data = await self._detect_saas_stack(domain)
            
            if saas_data['monthly_cost'] < 80:
                self.stats['stage_1_eliminated'] += 1
                print(f"  ‚ùå Stage 1 elimination: SaaS cost ${saas_data['monthly_cost']} < $80")
                return None
            
            print(f"  ‚úÖ Stage 1 passed: SaaS cost ${saas_data['monthly_cost']}")
            
            # Stage 2: Subscription Revenue Analysis (25% weight)
            subscription_data = await self._analyze_subscription_revenue(domain)
            
            combined_revenue = saas_data['monthly_cost'] + subscription_data['subscription_revenue']
            if combined_revenue < 150:  # Combined threshold
                self.stats['stage_2_eliminated'] += 1
                print(f"  ‚ùå Stage 2 elimination: Combined revenue ${combined_revenue} < $150")
                return None
            
            print(f"  ‚úÖ Stage 2 passed: Subscription revenue ${subscription_data['subscription_revenue']}")
            
            # Stage 3: Performance Revenue Impact (20% weight)
            performance_data = await self._calculate_performance_impact(domain)
            
            total_potential = combined_revenue + performance_data['performance_loss']
            if total_potential < 200:  # Total potential threshold
                self.stats['stage_3_eliminated'] += 1
                print(f"  ‚ùå Stage 3 elimination: Total potential ${total_potential} < $200")
                return None
            
            print(f"  ‚úÖ Stage 3 passed: Performance loss ${performance_data['performance_loss']}")
            
            # Stage 4: Enhanced Intelligence (for high-potential prospects)
            if total_potential >= 400:  # High-value prospects get full analysis
                ad_data = await self._detect_ad_spend(domain)
                authority_data = await self._analyze_domain_authority(domain)
                print(f"  üéØ Enhanced analysis: Ad spend ${ad_data['estimated_ad_spend']}")
            else:
                ad_data = {'active_ads': False, 'estimated_ad_spend': 0, 'ad_platforms': []}
                authority_data = {'domain_age_months': 12, 'authority_score': 50, 'follower_count': 0}
            
            # Calculate financial signals
            financial_signals = FinancialSignals(
                domain=domain,
                # SaaS Intelligence
                saas_tools=saas_data['tools'],
                saas_monthly_cost=saas_data['monthly_cost'],
                saas_optimization_potential=saas_data['optimization_potential'],
                # Subscription Intelligence
                subscription_revenue=subscription_data['subscription_revenue'],
                subscription_type=subscription_data['subscription_type'],
                subscription_platform=subscription_data['platform'],
                # Performance Intelligence
                performance_loss=performance_data['performance_loss'],
                performance_score=performance_data['performance_score'],
                js_bytes=performance_data['js_bytes'],
                total_requests=performance_data['total_requests'],
                # Ad Intelligence
                active_ads=ad_data['active_ads'],
                estimated_ad_spend=ad_data['estimated_ad_spend'],
                ad_platforms=ad_data['ad_platforms'],
                # Authority Intelligence
                domain_age_months=authority_data['domain_age_months'],
                authority_score=authority_data['authority_score'],
                follower_count=authority_data['follower_count'],
                # Calculated Metrics
                leak_score=0,  # Will be calculated
                priority='',   # Will be calculated
                total_potential_savings=0,  # Will be calculated
                processing_time=time.time() - start_time
            )
            
            # Calculate leak score and priority
            financial_signals.leak_score = self._calculate_leak_score(financial_signals)
            financial_signals.priority = self._classify_priority(financial_signals)
            financial_signals.total_potential_savings = (
                financial_signals.saas_optimization_potential +
                financial_signals.subscription_revenue * 0.3 +  # 30% optimization potential
                financial_signals.performance_loss +
                financial_signals.estimated_ad_spend * 0.2  # 20% ad optimization
            )
            
            processing_time = time.time() - start_time
            self.stats['average_processing_time'] = (
                (self.stats['average_processing_time'] * (self.stats['total_processed'] - 1) + processing_time) /
                self.stats['total_processed']
            )
            
            if financial_signals.leak_score >= 75:
                self.stats['pipeline_qualified'] += 1
                print(f"  üéØ QUALIFIED: Score {financial_signals.leak_score}, Priority {financial_signals.priority}")
                print(f"  üí∞ Total savings potential: ${financial_signals.total_potential_savings:,}")
                return financial_signals
            else:
                print(f"  ‚ùå Score {financial_signals.leak_score} < 75 threshold")
                return None
                
        except Exception as e:
            self.stats['api_errors'] += 1
            print(f"  ‚ùå Processing error: {e}")
            return None

    async def _detect_saas_stack(self, domain: str) -> Dict:
        """Detect SaaS tools using Wappalyzer CLI"""
        
        try:
            # Run Wappalyzer CLI
            result = subprocess.run(
                ['wappalyzer', '--urls', f'https://{domain}', '--output', 'json'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                technologies = data[0].get('technologies', [])
                
                detected_tools = []
                total_cost = 0
                total_optimization = 0
                
                for tech in technologies:
                    tech_name = tech.get('name', '').lower().replace(' ', '_')
                    
                    # Map to vendor costs
                    if tech_name in self.vendor_costs:
                        vendor_info = self.vendor_costs[tech_name]
                        detected_tools.append(tech_name)
                        total_cost += vendor_info['monthly_cost']
                        total_optimization += vendor_info['monthly_cost'] * vendor_info['optimization_potential']
                
                return {
                    'tools': detected_tools,
                    'monthly_cost': total_cost,
                    'optimization_potential': int(total_optimization)
                }
        
        except Exception as e:
            print(f"    Wappalyzer error: {e}")
        
        return {'tools': [], 'monthly_cost': 0, 'optimization_potential': 0}

    async def _analyze_subscription_revenue(self, domain: str) -> Dict:
        """Analyze Shopify subscription revenue via Storefront API"""
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                # Check Shopify cart.js endpoint
                cart_url = f"https://{domain}/cart.js"
                async with session.get(cart_url) as response:
                    if response.status == 200:
                        cart_data = await response.json()
                        return self._process_shopify_subscriptions(cart_data)
                
                # Fallback: Check for subscription indicators in main page
                async with session.get(f"https://{domain}") as response:
                    if response.status == 200:
                        content = await response.text()
                        return self._detect_subscription_signals(content)
        
        except Exception as e:
            print(f"    Subscription analysis error: {e}")
        
        return {'subscription_revenue': 0, 'subscription_type': 'none', 'platform': 'unknown'}

    def _process_shopify_subscriptions(self, cart_data: Dict) -> Dict:
        """Process Shopify cart data for subscription information"""
        
        subscription_revenue = 0
        subscription_types = []
        platform = 'shopify'
        
        for item in cart_data.get('items', []):
            if item.get('selling_plan_allocation'):
                # Shopify native subscriptions
                monthly_price = item['price'] / 100  # Shopify stores price in cents
                subscription_revenue += monthly_price
                subscription_types.append('shopify_native')
        
        # Check for subscription app indicators
        cart_str = str(cart_data).lower()
        if 'recharge' in cart_str:
            platform = 'recharge'
            subscription_revenue = max(subscription_revenue, 60)  # Minimum ReCharge value
        elif 'bold' in cart_str:
            platform = 'bold'
            subscription_revenue = max(subscription_revenue, 80)  # Minimum Bold value
        
        return {
            'subscription_revenue': int(subscription_revenue * 30),  # Monthly estimate
            'subscription_type': subscription_types[0] if subscription_types else 'detected',
            'platform': platform
        }

    def _detect_subscription_signals(self, content: str) -> Dict:
        """Detect subscription signals from page content"""
        
        content_lower = content.lower()
        subscription_signals = [
            'subscribe', 'subscription', 'recurring', 'monthly delivery',
            'auto-renew', 'membership', 'recharge', 'bold subscriptions'
        ]
        
        detected_signals = [signal for signal in subscription_signals if signal in content_lower]
        
        if detected_signals:
            # Estimate subscription revenue based on signals
            base_revenue = len(detected_signals) * 20  # $20 per signal
            
            # Platform detection
            if 'recharge' in content_lower:
                platform = 'recharge'
                base_revenue = max(base_revenue, 60)
            elif 'bold' in content_lower:
                platform = 'bold'
                base_revenue = max(base_revenue, 80)
            else:
                platform = 'unknown'
            
            return {
                'subscription_revenue': base_revenue,
                'subscription_type': 'detected',
                'platform': platform
            }
        
        return {'subscription_revenue': 0, 'subscription_type': 'none', 'platform': 'unknown'}

    async def _calculate_performance_impact(self, domain: str) -> Dict:
        """Calculate performance impact on revenue using multiple methods"""
        
        # Method 1: HTTP Archive (if available)
        if self.bigquery_client:
            archive_data = await self._query_http_archive(domain)
            if archive_data:
                return self._calculate_performance_revenue_loss(archive_data)
        
        # Method 2: Direct performance check
        performance_data = await self._direct_performance_check(domain)
        return self._estimate_performance_impact(performance_data)

    async def _query_http_archive(self, domain: str) -> Optional[Dict]:
        """Query HTTP Archive BigQuery for performance data"""
        
        try:
            query = f"""
            SELECT 
                bytesJS as js_bytes,
                bytesCSS as css_bytes,
                reqTotal as total_requests,
                TTFB as ttfb
            FROM `httparchive.summary_pages.2024_01_01_mobile`
            WHERE url LIKE '%{domain}%'
            LIMIT 1
            """
            
            job = self.bigquery_client.query(query)
            results = job.result()
            
            for row in results:
                return {
                    'js_bytes': row.js_bytes or 0,
                    'css_bytes': row.css_bytes or 0,
                    'total_requests': row.total_requests or 0,
                    'ttfb': row.ttfb or 0
                }
        
        except Exception as e:
            print(f"    BigQuery error: {e}")
        
        return None

    async def _direct_performance_check(self, domain: str) -> Dict:
        """Direct performance check via HTTP request analysis"""
        
        try:
            start_time = time.time()
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get(f"https://{domain}") as response:
                    if response.status == 200:
                        content = await response.text()
                        load_time = time.time() - start_time
                        
                        # Analyze content for performance signals
                        js_count = content.count('<script')
                        css_count = content.count('<link rel="stylesheet"')
                        img_count = content.count('<img')
                        
                        return {
                            'load_time': load_time,
                            'js_count': js_count,
                            'css_count': css_count,
                            'img_count': img_count,
                            'content_size': len(content)
                        }
        except Exception as e:
            print(f"    Performance check error: {e}")
        
        return {'load_time': 3.0, 'js_count': 10, 'css_count': 5, 'img_count': 20, 'content_size': 100000}

    def _calculate_performance_revenue_loss(self, archive_data: Dict) -> Dict:
        """Calculate revenue loss from HTTP Archive data"""
        
        js_bytes = archive_data['js_bytes']
        css_bytes = archive_data['css_bytes']
        total_requests = archive_data['total_requests']
        
        # Performance scoring (0-100)
        bloat_penalty = min((js_bytes + css_bytes) / 1000000 * 10, 50)  # Max 50 point penalty
        request_penalty = min(max(0, total_requests - 50) * 0.5, 30)   # Max 30 point penalty
        
        performance_score = max(0, 100 - bloat_penalty - request_penalty)
        
        # Revenue impact calculation
        if performance_score < 70:
            # Conservative revenue impact model
            monthly_visitors = 5000  # Conservative estimate
            baseline_conversion = 0.02  # 2% baseline
            avg_order_value = 75
            
            # Performance impact on conversion
            conversion_loss_factor = (70 - performance_score) / 100 * 0.4  # Max 40% loss
            monthly_loss = monthly_visitors * baseline_conversion * avg_order_value * conversion_loss_factor
            
            return {
                'performance_loss': int(monthly_loss),
                'performance_score': int(performance_score),
                'js_bytes': js_bytes,
                'total_requests': total_requests
            }
        
        return {
            'performance_loss': 0,
            'performance_score': int(performance_score),
            'js_bytes': js_bytes,
            'total_requests': total_requests
        }

    def _estimate_performance_impact(self, perf_data: Dict) -> Dict:
        """Estimate performance impact from direct check"""
        
        load_time = perf_data['load_time']
        js_count = perf_data['js_count']
        content_size = perf_data['content_size']
        
        # Performance scoring
        load_penalty = min(max(0, load_time - 2) * 20, 40)  # Penalty for >2s load time
        js_penalty = min(max(0, js_count - 10) * 2, 20)     # Penalty for >10 JS files
        size_penalty = min(content_size / 100000 * 10, 30)  # Size penalty
        
        performance_score = max(0, 100 - load_penalty - js_penalty - size_penalty)
        
        # Revenue impact estimate
        if performance_score < 60:
            estimated_loss = (60 - performance_score) * 5  # $5 per performance point lost
            return {
                'performance_loss': int(estimated_loss),
                'performance_score': int(performance_score),
                'js_bytes': content_size,  # Approximate
                'total_requests': js_count + perf_data['css_count'] + perf_data['img_count']
            }
        
        return {
            'performance_loss': 0,
            'performance_score': int(performance_score),
            'js_bytes': content_size,
            'total_requests': js_count + perf_data['css_count'] + perf_data['img_count']
        }

    async def _detect_ad_spend(self, domain: str) -> Dict:
        """Detect active ad spend via Meta Ad Library"""
        
        if not self.meta_token or not self._check_rate_limit('meta_ads'):
            return {'active_ads': False, 'estimated_ad_spend': 0, 'ad_platforms': []}
        
        try:
            company_name = domain.split('.')[0].replace('-', ' ')
            
            url = "https://graph.facebook.com/v18.0/ads_archive"
            params = {
                'access_token': self.meta_token,
                'search_terms': company_name,
                'ad_reached_countries': 'BR',
                'ad_active_status': 'ACTIVE',
                'limit': 10
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        ads = data.get('data', [])
                        
                        self.api_calls['meta_ads']['count'] += 1
                        
                        if ads:
                            # Estimate spend based on ad count and recency
                            estimated_spend = len(ads) * 300  # $300 per active ad (conservative)
                            return {
                                'active_ads': True,
                                'estimated_ad_spend': estimated_spend,
                                'ad_platforms': ['meta']
                            }
        
        except Exception as e:
            print(f"    Ad detection error: {e}")
        
        return {'active_ads': False, 'estimated_ad_spend': 0, 'ad_platforms': []}

    async def _analyze_domain_authority(self, domain: str) -> Dict:
        """Analyze domain authority via RDAP and social signals"""
        
        authority_data = await self._get_rdap_data(domain)
        social_data = await self._get_social_signals(domain)
        
        return {
            'domain_age_months': authority_data.get('domain_age_months', 12),
            'authority_score': authority_data.get('authority_score', 50),
            'follower_count': social_data.get('follower_count', 0)
        }

    async def _get_rdap_data(self, domain: str) -> Dict:
        """Get domain data via RDAP"""
        
        try:
            rdap_url = f"https://rdap.org/domain/{domain}"
            async with aiohttp.ClientSession() as session:
                async with session.get(rdap_url) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # Extract creation date
                        events = data.get('events', [])
                        creation_event = next((e for e in events if e.get('eventAction') == 'registration'), None)
                        
                        if creation_event:
                            creation_date = datetime.fromisoformat(creation_event['eventDate'].replace('Z', '+00:00'))
                            domain_age = (datetime.now(creation_date.tzinfo) - creation_date).days / 30
                            
                            # Authority scoring based on age
                            authority_score = min(domain_age * 3, 100)  # 3 points per month, max 100
                            
                            return {
                                'domain_age_months': int(domain_age),
                                'authority_score': int(authority_score)
                            }
        
        except Exception as e:
            print(f"    RDAP error: {e}")
        
        return {'domain_age_months': 12, 'authority_score': 50}

    async def _get_social_signals(self, domain: str) -> Dict:
        """Extract social signals (Instagram followers) from website"""
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"https://{domain}") as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # Extract Instagram handle
                        import re
                        instagram_pattern = r'instagram\.com/([a-zA-Z0-9_.]+)'
                        match = re.search(instagram_pattern, content)
                        
                        if match and self._check_rate_limit('instagram'):
                            handle = match.group(1)
                            follower_count = await self._get_instagram_followers(handle)
                            return {'follower_count': follower_count}
        
        except Exception as e:
            print(f"    Social signals error: {e}")
        
        return {'follower_count': 0}

    async def _get_instagram_followers(self, handle: str) -> int:
        """Get Instagram follower count via oEmbed API"""
        
        try:
            url = "https://graph.facebook.com/v18.0/instagram_oembed"
            params = {'url': f'https://instagram.com/{handle}'}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.api_calls['instagram']['count'] += 1
                        return data.get('author_followers_count', 0)
        
        except Exception as e:
            print(f"    Instagram API error: {e}")
        
        return 0

    def _check_rate_limit(self, api_name: str) -> bool:
        """Check if API rate limit allows request"""
        
        limits = {
            'meta_ads': 100,     # 100 requests per hour
            'instagram': 200,    # 200 requests per hour
            'bigquery': 1000     # 1TB per day (1000 GB)
        }
        
        api_info = self.api_calls.get(api_name, {})
        
        if datetime.now() > api_info.get('reset_time', datetime.now()):
            # Reset counter
            self.api_calls[api_name] = {
                'count': 0,
                'reset_time': datetime.now() + timedelta(hours=1)
            }
            return True
        
        return api_info.get('count', 0) < limits.get(api_name, 100)

    def _calculate_leak_score(self, signals: FinancialSignals) -> int:
        """Calculate comprehensive leak score (0-100)"""
        
        score = 0
        
        # SaaS waste (40% weight)
        saas_score = min(signals.saas_monthly_cost / 20, 40)  # $20 = 1 point, max 40
        score += saas_score
        
        # Subscription revenue (25% weight)
        subscription_score = min(signals.subscription_revenue / 25, 25)  # $25 = 1 point, max 25
        score += subscription_score
        
        # Performance impact (20% weight)
        performance_score = min(signals.performance_loss / 15, 20)  # $15 = 1 point, max 20
        score += performance_score
        
        # Active advertising (10% weight)
        if signals.active_ads:
            ad_score = min(signals.estimated_ad_spend / 100, 10)  # $100 = 1 point, max 10
            score += ad_score
        
        # Domain authority (5% weight)
        authority_score = min(signals.authority_score / 20, 5)  # 20 authority = 1 point, max 5
        score += authority_score
        
        return min(int(score), 100)

    def _classify_priority(self, signals: FinancialSignals) -> str:
        """Classify prospect priority based on financial signals"""
        
        total_potential = signals.total_potential_savings
        leak_score = signals.leak_score
        
        if leak_score >= 90 and total_potential >= 1000:
            return 'IMMEDIATE'
        elif leak_score >= 80 and total_potential >= 500:
            return 'HIGH'
        elif leak_score >= 75 and total_potential >= 200:
            return 'MEDIUM'
        else:
            return 'LOW'

    def get_processing_stats(self) -> Dict:
        """Get processing statistics"""
        
        total = self.stats['total_processed']
        if total == 0:
            return self.stats
        
        elimination_rate = (
            (self.stats['stage_1_eliminated'] + 
             self.stats['stage_2_eliminated'] + 
             self.stats['stage_3_eliminated']) / total * 100
        )
        
        qualification_rate = self.stats['pipeline_qualified'] / total * 100
        
        return {
            **self.stats,
            'elimination_rate': f"{elimination_rate:.1f}%",
            'qualification_rate': f"{qualification_rate:.1f}%",
            'domains_per_hour': int(3600 / self.stats['average_processing_time']) if self.stats['average_processing_time'] > 0 else 0
        }

# Production Demo
async def demo_financial_orchestrator():
    """Demo the financial intelligence orchestrator"""
    
    print("\nüéØ FINANCIAL SIGNAL ORCHESTRATOR DEMO")
    print("=" * 70)
    
    orchestrator = FinancialSignalOrchestrator()
    
    # Test domains (mix of potential prospects)
    test_domains = [
        'exemplo-beauty.com.br',  # Beauty/skincare
        'exemplo-saas.io',        # B2B SaaS
        'exemplo-ecommerce.com.br', # E-commerce
        'google.com'              # Should be filtered out
    ]
    
    qualified_prospects = []
    
    for domain in test_domains:
        print(f"\n" + "="*50)
        result = await orchestrator.process_domain_financial_cascade(domain)
        
        if result:
            qualified_prospects.append(result)
    
    # Show results
    print(f"\nüèÜ ORCHESTRATOR RESULTS")
    print("=" * 50)
    
    stats = orchestrator.get_processing_stats()
    print(f"üìä Processing Statistics:")
    print(f"  ‚Ä¢ Total processed: {stats['total_processed']}")
    print(f"  ‚Ä¢ Pipeline qualified: {stats['pipeline_qualified']}")
    print(f"  ‚Ä¢ Elimination rate: {stats['elimination_rate']}")
    print(f"  ‚Ä¢ Avg processing time: {stats['average_processing_time']:.2f}s")
    print(f"  ‚Ä¢ Domains per hour: {stats['domains_per_hour']}")
    
    if qualified_prospects:
        print(f"\nüéØ QUALIFIED PROSPECTS:")
        for prospect in qualified_prospects:
            print(f"  üè¢ {prospect.domain}")
            print(f"     Score: {prospect.leak_score}/100")
            print(f"     Priority: {prospect.priority}")
            print(f"     Savings: ${prospect.total_potential_savings:,}")
            print(f"     SaaS cost: ${prospect.saas_monthly_cost}")
            print(f"     Subscription: ${prospect.subscription_revenue}")
            print(f"     Performance loss: ${prospect.performance_loss}")
            print()
    
    print("‚úÖ Financial Intelligence Orchestrator operational!")

if __name__ == "__main__":
    asyncio.run(demo_financial_orchestrator())
