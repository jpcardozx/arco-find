#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARCO Deep Technical Audit Engine - OJambu Bags
An√°lise t√©cnica profunda com APIs gratuitas para identificar problemas concretos
que est√£o prejudicando performance, SEO, convers√£o e receita
"""

import requests
import json
import time
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse
import subprocess
import os
from dataclasses import dataclass
from typing import Dict, List, Optional

@dataclass
class TechnicalIssue:
    """Estrutura para problemas t√©cnicos identificados"""
    category: str
    severity: str  # critical, high, medium, low
    issue: str
    impact: str
    evidence: str
    cost_estimate: str
    solution: str
    business_justification: str

class DeepTechnicalAuditor:
    """Auditor t√©cnico profundo usando APIs gratuitas e an√°lises avan√ßadas"""
    
    def __init__(self, target_url: str):
        self.target_url = target_url
        self.domain = urlparse(target_url).netloc
        self.issues = []
        self.performance_data = {}
        self.seo_data = {}
        self.security_data = {}
        self.technical_data = {}
        
    def run_comprehensive_audit(self) -> Dict:
        """Executa auditoria t√©cnica completa"""
        
        print(f"üîç ARCO DEEP TECHNICAL AUDIT")
        print(f"Target: {self.target_url}")
        print(f"Objective: Identify revenue-impacting technical issues")
        print("=" * 70)
        
        audit_results = {
            'audit_metadata': {
                'target_url': self.target_url,
                'domain': self.domain,
                'audit_date': datetime.now().isoformat(),
                'methodology': 'comprehensive_technical_analysis',
                'tools_used': []
            },
            'critical_issues': [],
            'performance_analysis': {},
            'seo_technical_audit': {},
            'security_assessment': {},
            'mobile_optimization': {},
            'conversion_blockers': {},
            'cost_analysis': {},
            'competitive_gaps': {},
            'action_plan': {}
        }
        
        # 1. Performance profunda com Core Web Vitals
        print("\nüöÄ 1. PERFORMANCE & CORE WEB VITALS ANALYSIS")
        self.performance_data = self._analyze_performance_deep()
        audit_results['performance_analysis'] = self.performance_data
        
        # 2. Auditoria t√©cnica de SEO
        print("\nüîç 2. TECHNICAL SEO AUDIT")
        self.seo_data = self._audit_technical_seo()
        audit_results['seo_technical_audit'] = self.seo_data
        
        # 3. An√°lise de seguran√ßa e headers
        print("\nüõ°Ô∏è 3. SECURITY & HEADERS ANALYSIS")
        self.security_data = self._analyze_security_headers()
        audit_results['security_assessment'] = self.security_data
        
        # 4. Mobile e responsividade
        print("\nüì± 4. MOBILE OPTIMIZATION ANALYSIS")
        mobile_data = self._analyze_mobile_optimization()
        audit_results['mobile_optimization'] = mobile_data
        
        # 5. Conversion blockers
        print("\nüí∞ 5. CONVERSION BLOCKERS ANALYSIS")
        conversion_data = self._identify_conversion_blockers()
        audit_results['conversion_blockers'] = conversion_data
        
        # 6. An√°lise competitiva t√©cnica
        print("\nüèÜ 6. COMPETITIVE TECHNICAL ANALYSIS")
        competitive_data = self._analyze_competitive_gaps()
        audit_results['competitive_gaps'] = competitive_data
        
        # 7. C√°lculo de custos e ROI
        print("\nüí∏ 7. COST IMPACT ANALYSIS")
        cost_data = self._calculate_cost_impact()
        audit_results['cost_analysis'] = cost_data
        
        # 8. Consolida issues cr√≠ticos
        audit_results['critical_issues'] = self._consolidate_critical_issues()
        
        # 9. Plano de a√ß√£o priorizado
        audit_results['action_plan'] = self._generate_action_plan()
        
        return audit_results
    
    def _analyze_performance_deep(self) -> Dict:
        """An√°lise profunda de performance com m√©tricas espec√≠ficas"""
        
        performance = {
            'core_web_vitals': {},
            'loading_performance': {},
            'resource_analysis': {},
            'caching_analysis': {},
            'compression_analysis': {},
            'critical_issues': []
        }
        
        try:
            # Teste de velocidade com m√∫ltiplas m√©tricas
            print("  üìä Analyzing loading performance...")
            
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            
            # Teste de TTFB (Time to First Byte)
            start_time = time.time()
            response = requests.get(self.target_url, headers=headers, timeout=30)
            ttfb = time.time() - start_time
            
            # An√°lise de headers de cache
            cache_control = response.headers.get('cache-control', '')
            expires = response.headers.get('expires', '')
            etag = response.headers.get('etag', '')
            
            # An√°lise de compress√£o
            content_encoding = response.headers.get('content-encoding', '')
            content_length = len(response.content)
            
            # An√°lise de recursos na p√°gina
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Conta recursos
            images = soup.find_all('img')
            scripts = soup.find_all('script')
            stylesheets = soup.find_all('link', rel='stylesheet')
            
            # Identifica imagens sem otimiza√ß√£o
            large_images = []
            images_without_alt = 0
            for img in images:
                if not img.get('alt'):
                    images_without_alt += 1
                src = img.get('src', '')
                if src and not any(ext in src.lower() for ext in ['.webp', '.avif']):
                    large_images.append(src)
            
            # Scripts bloqueando renderiza√ß√£o
            blocking_scripts = []
            for script in scripts:
                if not script.get('async') and not script.get('defer'):
                    blocking_scripts.append(script.get('src', 'inline'))
            
            performance['loading_performance'] = {
                'ttfb_seconds': round(ttfb, 3),
                'total_page_size_bytes': content_length,
                'total_page_size_mb': round(content_length / (1024*1024), 2),
                'images_count': len(images),
                'scripts_count': len(scripts),
                'stylesheets_count': len(stylesheets)
            }
            
            performance['resource_analysis'] = {
                'unoptimized_images_count': len(large_images),
                'images_without_alt': images_without_alt,
                'blocking_scripts_count': len(blocking_scripts),
                'blocking_scripts': blocking_scripts[:5]  # Top 5
            }
            
            performance['caching_analysis'] = {
                'cache_control_present': bool(cache_control),
                'cache_control_value': cache_control,
                'expires_header_present': bool(expires),
                'etag_present': bool(etag)
            }
            
            performance['compression_analysis'] = {
                'compression_enabled': bool(content_encoding),
                'compression_type': content_encoding,
                'uncompressed_size_mb': round(content_length / (1024*1024), 2)
            }
            
            # Identifica problemas cr√≠ticos de performance
            if ttfb > 2.0:
                self.issues.append(TechnicalIssue(
                    category="Performance",
                    severity="critical",
                    issue=f"TTFB muito alto: {ttfb:.2f}s",
                    impact="Perda de 20-40% dos visitantes por lentid√£o",
                    evidence=f"TTFB medido: {ttfb:.2f}s (recomendado: <1s)",
                    cost_estimate="R$ 2.000-5.000/m√™s em vendas perdidas",
                    solution="Otimiza√ß√£o de servidor, CDN, cache din√¢mico",
                    business_justification="Sites com TTFB >2s perdem 40% dos visitantes"
                ))
            
            if content_length > 5 * 1024 * 1024:  # >5MB
                self.issues.append(TechnicalIssue(
                    category="Performance",
                    severity="high",
                    issue=f"P√°gina muito pesada: {content_length/(1024*1024):.1f}MB",
                    impact="Lentid√£o em mobile, alta taxa de abandono",
                    evidence=f"Tamanho da p√°gina: {content_length/(1024*1024):.1f}MB",
                    cost_estimate="R$ 1.500-3.000/m√™s em convers√µes perdidas",
                    solution="Compress√£o de imagens, lazy loading, minifica√ß√£o",
                    business_justification="P√°ginas >3MB t√™m 50% mais abandono"
                ))
            
            if len(large_images) > 5:
                self.issues.append(TechnicalIssue(
                    category="Performance",
                    severity="high",
                    issue=f"{len(large_images)} imagens n√£o otimizadas",
                    impact="Carregamento lento, consumo excessivo de dados",
                    evidence=f"Imagens sem WebP/AVIF: {len(large_images)}",
                    cost_estimate="R$ 800-1.500/m√™s em performance perdida",
                    solution="Convers√£o para WebP/AVIF, compress√£o autom√°tica",
                    business_justification="Otimiza√ß√£o de imagens melhora velocidade em 30-50%"
                ))
                
        except Exception as e:
            performance['error'] = str(e)
            print(f"  ‚ùå Error in performance analysis: {e}")
        
        return performance
    
    def _audit_technical_seo(self) -> Dict:
        """Auditoria t√©cnica completa de SEO"""
        
        seo = {
            'meta_tags_analysis': {},
            'structured_data': {},
            'internal_linking': {},
            'url_structure': {},
            'indexability': {},
            'critical_issues': []
        }
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(self.target_url, headers=headers, timeout=15)
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # An√°lise de meta tags
            title = soup.find('title')
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            canonical = soup.find('link', attrs={'rel': 'canonical'})
            
            # Open Graph
            og_title = soup.find('meta', attrs={'property': 'og:title'})
            og_desc = soup.find('meta', attrs={'property': 'og:description'})
            og_image = soup.find('meta', attrs={'property': 'og:image'})
            
            # Schema markup
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            microdata = soup.find_all(attrs={'itemscope': True})
            
            # Headers hierarchy
            h1_tags = soup.find_all('h1')
            h2_tags = soup.find_all('h2')
            h3_tags = soup.find_all('h3')
            
            # Links internos e externos
            all_links = soup.find_all('a', href=True)
            internal_links = []
            external_links = []
            
            for link in all_links:
                href = link['href']
                if href.startswith('http'):
                    if self.domain in href:
                        internal_links.append(href)
                    else:
                        external_links.append(href)
                elif href.startswith('/'):
                    internal_links.append(href)
            
            seo['meta_tags_analysis'] = {
                'title_present': bool(title),
                'title_length': len(title.get_text()) if title else 0,
                'meta_description_present': bool(meta_desc),
                'meta_description_length': len(meta_desc.get('content', '')) if meta_desc else 0,
                'canonical_present': bool(canonical),
                'og_tags_present': bool(og_title and og_desc),
                'og_image_present': bool(og_image)
            }
            
            seo['structured_data'] = {
                'json_ld_present': len(json_ld_scripts) > 0,
                'json_ld_count': len(json_ld_scripts),
                'microdata_present': len(microdata) > 0,
                'microdata_count': len(microdata)
            }
            
            seo['internal_linking'] = {
                'internal_links_count': len(internal_links),
                'external_links_count': len(external_links),
                'h1_count': len(h1_tags),
                'h2_count': len(h2_tags),
                'h3_count': len(h3_tags)
            }
            
            # Identifica problemas SEO cr√≠ticos
            if not title or len(title.get_text()) < 30:
                self.issues.append(TechnicalIssue(
                    category="SEO",
                    severity="critical",
                    issue="Title tag ausente ou muito curto",
                    impact="Perda de ranking e CTR no Google",
                    evidence=f"Title length: {len(title.get_text()) if title else 0} characters",
                    cost_estimate="R$ 3.000-8.000/m√™s em tr√°fego org√¢nico perdido",
                    solution="Implementar titles otimizadas com 50-60 caracteres",
                    business_justification="Titles otimizadas aumentam CTR em 15-25%"
                ))
            
            if not meta_desc or len(meta_desc.get('content', '')) < 120:
                self.issues.append(TechnicalIssue(
                    category="SEO",
                    severity="high",
                    issue="Meta description ausente ou muito curta",
                    impact="CTR baixo nos resultados de busca",
                    evidence=f"Meta desc length: {len(meta_desc.get('content', '')) if meta_desc else 0} chars",
                    cost_estimate="R$ 1.500-4.000/m√™s em clicks perdidos",
                    solution="Meta descriptions de 150-160 caracteres otimizadas",
                    business_justification="Meta descriptions aumentam CTR em 10-20%"
                ))
            
            if len(h1_tags) != 1:
                self.issues.append(TechnicalIssue(
                    category="SEO",
                    severity="medium",
                    issue=f"Estrutura H1 incorreta: {len(h1_tags)} H1s encontrados",
                    impact="Confus√£o para crawlers, ranking prejudicado",
                    evidence=f"H1 tags found: {len(h1_tags)} (should be exactly 1)",
                    cost_estimate="R$ 500-1.200/m√™s em ranking perdido",
                    solution="Implementar exatamente 1 H1 por p√°gina",
                    business_justification="Estrutura correta de headers melhora ranking"
                ))
            
            if not json_ld_scripts and not microdata:
                self.issues.append(TechnicalIssue(
                    category="SEO",
                    severity="high",
                    issue="Schema markup ausente",
                    impact="Rich snippets perdidos, CTR reduzido",
                    evidence="No JSON-LD or microdata found",
                    cost_estimate="R$ 1.000-2.500/m√™s em featured snippets perdidos",
                    solution="Implementar Schema.org markup para produtos",
                    business_justification="Schema markup aumenta CTR em 20-30%"
                ))
                
        except Exception as e:
            seo['error'] = str(e)
            print(f"  ‚ùå Error in SEO analysis: {e}")
        
        return seo
    
    def _analyze_security_headers(self) -> Dict:
        """An√°lise de headers de seguran√ßa e configura√ß√µes"""
        
        security = {
            'ssl_analysis': {},
            'security_headers': {},
            'vulnerabilities': {},
            'critical_issues': []
        }
        
        try:
            # An√°lise SSL/TLS
            print("  üîí Analyzing SSL/TLS configuration...")
            
            # Teste b√°sico de SSL
            ssl_response = requests.get(self.target_url, timeout=10)
            security_headers = ssl_response.headers
            
            # Headers de seguran√ßa importantes
            security_headers_check = {
                'strict-transport-security': security_headers.get('strict-transport-security'),
                'content-security-policy': security_headers.get('content-security-policy'),
                'x-frame-options': security_headers.get('x-frame-options'),
                'x-content-type-options': security_headers.get('x-content-type-options'),
                'x-xss-protection': security_headers.get('x-xss-protection'),
                'referrer-policy': security_headers.get('referrer-policy')
            }
            
            # Server information
            server_header = security_headers.get('server', '')
            x_powered_by = security_headers.get('x-powered-by', '')
            
            security['security_headers'] = security_headers_check
            security['ssl_analysis'] = {
                'https_enabled': self.target_url.startswith('https://'),
                'hsts_enabled': bool(security_headers_check['strict-transport-security']),
                'server_info_exposed': bool(server_header or x_powered_by)
            }
            
            # Identifica vulnerabilidades de seguran√ßa
            missing_headers = [k for k, v in security_headers_check.items() if not v]
            
            if not security_headers_check['strict-transport-security']:
                self.issues.append(TechnicalIssue(
                    category="Security",
                    severity="high",
                    issue="HSTS header ausente",
                    impact="Vulnerabilidade a ataques man-in-the-middle",
                    evidence="Strict-Transport-Security header not found",
                    cost_estimate="Risco de perda de dados e confian√ßa",
                    solution="Implementar HSTS com preload",
                    business_justification="HSTS protege contra ataques SSL"
                ))
            
            if not security_headers_check['content-security-policy']:
                self.issues.append(TechnicalIssue(
                    category="Security",
                    severity="medium",
                    issue="CSP header ausente",
                    impact="Vulnerabilidade a XSS attacks",
                    evidence="Content-Security-Policy header not found",
                    cost_estimate="Risco de ataques XSS",
                    solution="Implementar Content Security Policy",
                    business_justification="CSP previne 90% dos ataques XSS"
                ))
            
            if server_header or x_powered_by:
                self.issues.append(TechnicalIssue(
                    category="Security",
                    severity="low",
                    issue="Informa√ß√µes do servidor expostas",
                    impact="Facilita reconnaissance para atacantes",
                    evidence=f"Server: {server_header}, X-Powered-By: {x_powered_by}",
                    cost_estimate="Risco de seguran√ßa elevado",
                    solution="Remover headers que exp√µem tecnologia",
                    business_justification="Security through obscurity"
                ))
                
        except Exception as e:
            security['error'] = str(e)
            print(f"  ‚ùå Error in security analysis: {e}")
        
        return security
    
    def _analyze_mobile_optimization(self) -> Dict:
        """An√°lise de otimiza√ß√£o mobile"""
        
        mobile = {
            'viewport_analysis': {},
            'responsive_design': {},
            'mobile_performance': {},
            'touch_optimization': {}
        }
        
        try:
            # User agent mobile
            mobile_headers = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)'}
            response = requests.get(self.target_url, headers=mobile_headers, timeout=15)
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Viewport meta tag
            viewport = soup.find('meta', attrs={'name': 'viewport'})
            
            # Media queries (aproxima√ß√£o)
            styles = soup.find_all('style')
            media_queries_found = 0
            for style in styles:
                if '@media' in str(style):
                    media_queries_found += 1
            
            # Links para CSS externos com media queries
            css_links = soup.find_all('link', rel='stylesheet')
            responsive_css = sum(1 for link in css_links if link.get('media'))
            
            mobile['viewport_analysis'] = {
                'viewport_tag_present': bool(viewport),
                'viewport_content': viewport.get('content', '') if viewport else ''
            }
            
            mobile['responsive_design'] = {
                'media_queries_found': media_queries_found,
                'responsive_css_links': responsive_css,
                'estimated_responsive': media_queries_found > 0 or responsive_css > 0
            }
            
            # Identifica problemas mobile cr√≠ticos
            if not viewport:
                self.issues.append(TechnicalIssue(
                    category="Mobile",
                    severity="critical",
                    issue="Viewport meta tag ausente",
                    impact="Site n√£o responsivo em mobile",
                    evidence="No viewport meta tag found",
                    cost_estimate="R$ 4.000-10.000/m√™s em vendas mobile perdidas",
                    solution="Implementar <meta name='viewport' content='width=device-width, initial-scale=1'>",
                    business_justification="70% do tr√°fego e-commerce √© mobile"
                ))
            
            if media_queries_found == 0 and responsive_css == 0:
                self.issues.append(TechnicalIssue(
                    category="Mobile",
                    severity="critical",
                    issue="Design n√£o responsivo detectado",
                    impact="Experi√™ncia ruim em mobile, alta taxa de abandono",
                    evidence="No media queries or responsive CSS found",
                    cost_estimate="R$ 5.000-15.000/m√™s em convers√µes mobile perdidas",
                    solution="Implementar design responsivo completo",
                    business_justification="Sites responsivos convertem 60% mais em mobile"
                ))
                
        except Exception as e:
            mobile['error'] = str(e)
            print(f"  ‚ùå Error in mobile analysis: {e}")
        
        return mobile
    
    def _identify_conversion_blockers(self) -> Dict:
        """Identifica blockers espec√≠ficos de convers√£o no e-commerce"""
        
        conversion = {
            'checkout_analysis': {},
            'product_pages': {},
            'trust_signals': {},
            'cart_optimization': {}
        }
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(self.target_url, headers=headers, timeout=15)
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            content = response.text.lower()
            
            # An√°lise de trust signals
            ssl_badges = len([img for img in soup.find_all('img') if 'ssl' in str(img) or 'secure' in str(img)])
            payment_methods = []
            
            payment_keywords = ['visa', 'mastercard', 'paypal', 'pix', 'boleto', 'americanexpress']
            for keyword in payment_keywords:
                if keyword in content:
                    payment_methods.append(keyword)
            
            # Elementos de urg√™ncia/escassez
            urgency_keywords = ['√∫ltimas', 'restam', 'promo√ß√£o', 'desconto', 'oferta']
            urgency_found = sum(1 for keyword in urgency_keywords if keyword in content)
            
            # An√°lise de formul√°rios (checkout/contato)
            forms = soup.find_all('form')
            checkout_forms = [f for f in forms if any(word in str(f).lower() for word in ['checkout', 'comprar', 'finalizar'])]
            
            conversion['trust_signals'] = {
                'ssl_badges_count': ssl_badges,
                'payment_methods_visible': payment_methods,
                'payment_methods_count': len(payment_methods),
                'urgency_elements': urgency_found
            }
            
            conversion['checkout_analysis'] = {
                'checkout_forms_found': len(checkout_forms),
                'total_forms': len(forms)
            }
            
            # Identifica conversion blockers
            if len(payment_methods) < 3:
                self.issues.append(TechnicalIssue(
                    category="Conversion",
                    severity="high",
                    issue=f"Poucos m√©todos de pagamento vis√≠veis: {len(payment_methods)}",
                    impact="Clientes abandonam por falta de op√ß√µes de pagamento",
                    evidence=f"Payment methods found: {payment_methods}",
                    cost_estimate="R$ 2.000-5.000/m√™s em checkouts abandonados",
                    solution="Exibir claramente todos os m√©todos: cart√µes, PIX, boleto",
                    business_justification="Mais op√ß√µes de pagamento aumentam convers√£o em 25%"
                ))
            
            if ssl_badges == 0:
                self.issues.append(TechnicalIssue(
                    category="Conversion",
                    severity="medium",
                    issue="Aus√™ncia de badges de seguran√ßa SSL",
                    impact="Redu√ß√£o de confian√ßa, abandono no checkout",
                    evidence="No SSL/security badges found on page",
                    cost_estimate="R$ 1.000-2.500/m√™s em convers√µes perdidas por desconfian√ßa",
                    solution="Adicionar badges SSL e certifica√ß√µes de seguran√ßa",
                    business_justification="Badges de seguran√ßa aumentam convers√£o em 15%"
                ))
            
            if urgency_found == 0:
                self.issues.append(TechnicalIssue(
                    category="Conversion",
                    severity="medium",
                    issue="Aus√™ncia de elementos de urg√™ncia/escassez",
                    impact="Falta de motiva√ß√£o para compra imediata",
                    evidence="No urgency/scarcity elements found",
                    cost_estimate="R$ 800-2.000/m√™s em convers√µes atrasadas",
                    solution="Implementar contadores, estoques limitados, ofertas por tempo",
                    business_justification="Urg√™ncia aumenta convers√£o imediata em 20%"
                ))
                
        except Exception as e:
            conversion['error'] = str(e)
            print(f"  ‚ùå Error in conversion analysis: {e}")
        
        return conversion
    
    def _analyze_competitive_gaps(self) -> Dict:
        """An√°lise de gaps competitivos t√©cnicos"""
        
        competitive = {
            'technology_stack': {},
            'feature_gaps': {},
            'performance_comparison': {},
            'seo_gaps': {}
        }
        
        # Competitors brasileiros de e-commerce de bolsas
        competitors = [
            'https://www.schutz.com.br',
            'https://www.anacapri.com.br',
            'https://www.arezzo.com.br'
        ]
        
        try:
            print("  üèÜ Analyzing competitive gaps...")
            
            competitor_features = []
            
            for competitor_url in competitors:
                try:
                    response = requests.get(competitor_url, timeout=10)
                    if response.status_code == 200:
                        content = response.text.lower()
                        
                        # Features avan√ßadas que competidores podem ter
                        features_check = {
                            'live_chat': 'chat' in content or 'atendimento' in content,
                            'wishlist': 'wishlist' in content or 'favoritos' in content,
                            'product_reviews': 'avalia√ß√£o' in content or 'review' in content,
                            'size_guide': 'guia de tamanhos' in content or 'tabela' in content,
                            'virtual_try_on': 'experimentar' in content or 'realidade' in content,
                            'recommendation_engine': 'recomendado' in content or 'voc√™ pode gostar' in content,
                            'social_proof': 'compraram junto' in content or 'outros clientes' in content
                        }
                        
                        competitor_features.append({
                            'competitor': competitor_url,
                            'features': features_check
                        })
                        
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Could not analyze {competitor_url}: {e}")
            
            # Analisa OJambu para compara√ß√£o
            ojambu_response = requests.get(self.target_url, timeout=10)
            ojambu_content = ojambu_response.text.lower()
            
            ojambu_features = {
                'live_chat': 'chat' in ojambu_content or 'atendimento' in ojambu_content,
                'wishlist': 'wishlist' in ojambu_content or 'favoritos' in ojambu_content,
                'product_reviews': 'avalia√ß√£o' in ojambu_content or 'review' in ojambu_content,
                'size_guide': 'guia de tamanhos' in ojambu_content or 'tabela' in ojambu_content,
                'virtual_try_on': 'experimentar' in ojambu_content or 'realidade' in ojambu_content,
                'recommendation_engine': 'recomendado' in ojambu_content or 'voc√™ pode gostar' in ojambu_content,
                'social_proof': 'compraram junto' in ojambu_content or 'outros clientes' in ojambu_content
            }
            
            # Identifica gaps
            missing_features = []
            for feature, has_feature in ojambu_features.items():
                if not has_feature:
                    # Verifica se competidores t√™m
                    competitors_with_feature = sum(1 for comp in competitor_features 
                                                 if comp['features'].get(feature, False))
                    if competitors_with_feature > 0:
                        missing_features.append({
                            'feature': feature,
                            'competitors_with_feature': competitors_with_feature,
                            'total_competitors': len(competitor_features)
                        })
            
            competitive['feature_gaps'] = {
                'ojambu_features': ojambu_features,
                'missing_features': missing_features,
                'competitive_analysis': competitor_features
            }
            
            # Identifica gaps cr√≠ticos
            critical_missing = [f for f in missing_features if f['competitors_with_feature'] >= len(competitor_features) * 0.5]
            
            for missing in critical_missing:
                feature_name = missing['feature'].replace('_', ' ').title()
                
                impact_map = {
                    'Live Chat': ('R$ 3.000-8.000/m√™s', 'Suporte imediato aumenta convers√£o em 30%'),
                    'Wishlist': ('R$ 1.500-4.000/m√™s', 'Wishlist gera 20% mais retorno de visitantes'),
                    'Product Reviews': ('R$ 2.000-6.000/m√™s', 'Reviews aumentam convers√£o em 25%'),
                    'Size Guide': ('R$ 1.000-3.000/m√™s', 'Guia de tamanhos reduz devolu√ß√µes em 40%'),
                    'Recommendation Engine': ('R$ 2.500-7.000/m√™s', 'Recomenda√ß√µes aumentam ticket m√©dio em 35%'),
                    'Social Proof': ('R$ 800-2.000/m√™s', 'Social proof aumenta confian√ßa e convers√£o')
                }
                
                cost_estimate, business_justification = impact_map.get(feature_name, ('R$ 500-1.500/m√™s', 'Melhora experi√™ncia do usu√°rio'))
                
                self.issues.append(TechnicalIssue(
                    category="Competitive",
                    severity="high" if missing['competitors_with_feature'] >= len(competitor_features) else "medium",
                    issue=f"Feature ausente: {feature_name}",
                    impact="Desvantagem competitiva, perda de vendas",
                    evidence=f"{missing['competitors_with_feature']}/{len(competitor_features)} competidores t√™m esta feature",
                    cost_estimate=cost_estimate,
                    solution=f"Implementar {feature_name} no e-commerce",
                    business_justification=business_justification
                ))
                
        except Exception as e:
            competitive['error'] = str(e)
            print(f"  ‚ùå Error in competitive analysis: {e}")
        
        return competitive
    
    def _calculate_cost_impact(self) -> Dict:
        """Calcula impacto financeiro dos problemas identificados"""
        
        cost_analysis = {
            'total_estimated_monthly_loss': 0,
            'issues_by_severity': {'critical': [], 'high': [], 'medium': [], 'low': []},
            'roi_projections': {},
            'implementation_costs': {}
        }
        
        # Agrupa issues por severidade
        for issue in self.issues:
            cost_analysis['issues_by_severity'][issue.severity].append(issue)
        
        # Calcula perda mensal estimada
        total_loss = 0
        for issue in self.issues:
            # Extrai valor monet√°rio da string (simplificado)
            cost_str = issue.cost_estimate
            if 'R$' in cost_str:
                import re
                numbers = re.findall(r'[\d.]+', cost_str.replace('.', ''))
                if len(numbers) >= 2:
                    # Pega o valor m√©dio do range
                    low = int(numbers[0])
                    high = int(numbers[1])
                    avg_loss = (low + high) / 2
                    total_loss += avg_loss
        
        cost_analysis['total_estimated_monthly_loss'] = total_loss
        
        # Projeta ROI de corre√ß√µes
        implementation_cost = total_loss * 0.5  # Estimativa de 50% da perda mensal
        monthly_recovery = total_loss * 0.7     # Recupera√ß√£o de 70% das perdas
        
        cost_analysis['roi_projections'] = {
            'estimated_implementation_cost': implementation_cost,
            'monthly_revenue_recovery': monthly_recovery,
            'payback_period_months': round(implementation_cost / monthly_recovery, 1) if monthly_recovery > 0 else float('inf'),
            'annual_profit_impact': monthly_recovery * 12 - implementation_cost
        }
        
        return cost_analysis
    
    def _consolidate_critical_issues(self) -> List[Dict]:
        """Consolida os issues mais cr√≠ticos para o relat√≥rio executivo"""
        
        # Ordena por severidade e impacto financeiro
        severity_order = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}
        
        sorted_issues = sorted(self.issues, 
                             key=lambda x: severity_order.get(x.severity, 0), 
                             reverse=True)
        
        # Converte para dict para serializa√ß√£o
        critical_issues = []
        for issue in sorted_issues[:10]:  # Top 10 issues
            critical_issues.append({
                'category': issue.category,
                'severity': issue.severity,
                'issue': issue.issue,
                'impact': issue.impact,
                'evidence': issue.evidence,
                'cost_estimate': issue.cost_estimate,
                'solution': issue.solution,
                'business_justification': issue.business_justification
            })
        
        return critical_issues
    
    def _generate_action_plan(self) -> Dict:
        """Gera plano de a√ß√£o priorizado"""
        
        action_plan = {
            'immediate_actions': [],    # 0-7 dias
            'short_term': [],          # 1-4 semanas  
            'medium_term': [],         # 1-3 meses
            'long_term': []            # 3+ meses
        }
        
        # Classifica a√ß√µes por urg√™ncia e complexidade
        for issue in self.issues:
            action = {
                'issue': issue.issue,
                'solution': issue.solution,
                'cost_estimate': issue.cost_estimate,
                'business_justification': issue.business_justification
            }
            
            # Classifica por categoria e severidade
            if issue.severity == 'critical':
                if issue.category in ['Performance', 'Mobile']:
                    action_plan['immediate_actions'].append(action)
                else:
                    action_plan['short_term'].append(action)
            elif issue.severity == 'high':
                if issue.category in ['SEO', 'Conversion']:
                    action_plan['short_term'].append(action)
                else:
                    action_plan['medium_term'].append(action)
            else:
                action_plan['long_term'].append(action)
        
        return action_plan
    
    def _make_serializable(self, obj):
        """Converte TechnicalIssue objects para formato serializ√°vel"""
        
        if isinstance(obj, TechnicalIssue):
            return {
                'category': obj.category,
                'severity': obj.severity,
                'issue': obj.issue,
                'impact': obj.impact,
                'evidence': obj.evidence,
                'cost_estimate': obj.cost_estimate,
                'solution': obj.solution,
                'business_justification': obj.business_justification
            }
        elif isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        else:
            return obj

def main():
    """Executa auditoria t√©cnica completa da OJambu"""
    
    print("üéØ ARCO DEEP TECHNICAL AUDIT - OJAMBU BAGS")
    print("Objective: Identify specific revenue-impacting technical issues")
    print("=" * 70)
    
    auditor = DeepTechnicalAuditor("https://ojambubags.com.br/")
    
    # Executa auditoria completa
    audit_results = auditor.run_comprehensive_audit()
      # Export results
    output_file = f"results/ojambu_technical_audit_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    # Convert TechnicalIssue objects to dicts for JSON serialization
    serializable_results = auditor._make_serializable(audit_results)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìÑ Complete Technical Audit: {output_file}")
    
    # Summary
    critical_count = len([i for i in auditor.issues if i.severity == 'critical'])
    high_count = len([i for i in auditor.issues if i.severity == 'high'])
    total_loss = audit_results['cost_analysis']['total_estimated_monthly_loss']
    
    print(f"\nüö® EXECUTIVE SUMMARY:")
    print(f"‚Ä¢ Critical Issues: {critical_count}")
    print(f"‚Ä¢ High Priority Issues: {high_count}")
    print(f"‚Ä¢ Estimated Monthly Loss: R$ {total_loss:,.0f}")
    print(f"‚Ä¢ ROI Implementation: {audit_results['cost_analysis']['roi_projections']['payback_period_months']} months payback")
    
    print(f"\n‚úÖ DEEP TECHNICAL AUDIT COMPLETE")

if __name__ == "__main__":
    main()
