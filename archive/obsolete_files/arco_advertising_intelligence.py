#!/usr/bin/env python3
"""
üéØ ARCO ADVERTISING INTELLIGENCE ENGINE
Sistema integrado que implementa a estrat√©gia "est√° anunciando agora" + prova de dor.

PIPELINE:
1. Places API (seed) ‚Üí Empresas locais qualificadas por nicho/regi√£o
2. Confirma√ß√£o de an√∫ncios ativos ‚Üí ATC/Meta/TikTok Ad Libraries  
3. Detec√ß√£o de sinais de dor ‚Üí Core Web Vitals + Tech Stack + Form Issues
4. Qualifica√ß√£o final ‚Üí Score USD 497 sprint viability

FOCO: AU/NZ SMBs com 3-10 an√∫ncios ativos + LCP>2.5s ou INP>400ms
"""

import requests
import json
import time
import logging
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from urllib.parse import urlparse, urljoin
import re
import os
from dotenv import load_dotenv

# Carregar environment variables do arquivo .env
load_dotenv()

# Configura√ß√£o
logging.basicConfig(
    level=logging.INFO,  # Voltando para INFO
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# APIs Configuration
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')  # REMOVIDA chave hardcoded
if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY environment variable is required")

# Places API (New) - Endpoints corretos
GOOGLE_PLACES_URL = "https://places.googleapis.com/v1"
PAGESPEED_API_URL = "https://www.googleapis.com/pagespeedonline/v5/runPagespeed"

@dataclass
class AdActivityIntel:
    """Intelig√™ncia de atividade publicit√°ria"""
    platform: str
    is_active: bool
    recent_ads_count: int
    latest_ad_date: Optional[str]
    creative_age_days: Optional[int]
    estimated_monthly_spend: Optional[float]
    confirmation_url: Optional[str]

@dataclass
class TechnicalPainSignal:
    """Sinal t√©cnico de dor (oportunidade de sprint)"""
    category: str  # 'web_vitals', 'tech_stack', 'forms', 'tracking', 'ssl'
    severity: str  # 'high', 'medium', 'low'
    metric_name: str
    current_value: float
    threshold_value: float
    sprint_suggestion: str
    estimated_impact: str

@dataclass
class QualifiedAdvertiser:
    """Lead qualificado: anuncia + tem dor t√©cnica mensur√°vel"""
    # Dados b√°sicos
    company_name: str
    website: str
    location: str
    phone: Optional[str]
    place_id: str
    
    # Atividade publicit√°ria confirmada
    ad_activity: List[AdActivityIntel]
    total_active_platforms: int
    estimated_total_monthly_spend: float
    
    # Sinais de dor t√©cnica
    pain_signals: List[TechnicalPainSignal]
    web_vitals_score: float
    technical_debt_score: float
    
    # Qualifica√ß√£o final
    qualification_score: float
    sprint_viability: str  # 'high', 'medium', 'low'
    suggested_sprint: str
    estimated_sprint_value: float
    
    # Prova para outreach
    proof_pack: Dict[str, str]
    
    # Metadados
    discovery_date: str
    confidence_level: str

class AdvertisingIntelligenceEngine:
    """Motor de intelig√™ncia publicit√°ria focado em 'anuncia agora' + dor t√©cnica"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Rate limiting
        self.request_delay = 1.0
        self.last_request_time = 0
        
        # Crit√©rios de qualifica√ß√£o
        self.min_qualification_score = 75
        self.target_regions = ['AU', 'NZ']  # Foco principal
        
        # Nichos receptivos para sprints USD 497
        self.target_verticals = {
            'dental': {
                'keywords': ['dental', 'dentist', 'orthodontics', 'dental clinic'],
                'sprint_focus': 'booking_optimization',
                'avg_sprint_value': 497
            },
            'aesthetic': {
                'keywords': ['aesthetic', 'cosmetic', 'beauty clinic', 'plastic surgery'],
                'sprint_focus': 'conversion_tracking',
                'avg_sprint_value': 497
            },
            'real_estate': {
                'keywords': ['real estate', 'property', 'realtor', 'homes'],
                'sprint_focus': 'lead_capture',
                'avg_sprint_value': 497
            },
            'legal': {
                'keywords': ['lawyer', 'legal', 'attorney', 'law firm'],
                'sprint_focus': 'contact_optimization',
                'avg_sprint_value': 497
            },
            'ecommerce': {
                'keywords': ['shop', 'store', 'retail', 'online store'],
                'sprint_focus': 'checkout_optimization',
                'avg_sprint_value': 497
            }
        }

    def _rate_limit(self):
        """Controle de rate limiting"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.request_delay:
            time.sleep(self.request_delay - time_since_last)
        
        self.last_request_time = time.time()

    def discover_local_businesses(self, vertical: str, location: str = "Australia") -> List[Dict]:
        """
        FASE 1: Places API (NEW) para seed de empresas locais
        Usa field mask obrigat√≥rio para minimizar custos
        """
        logger.info(f"üîç Descobrindo {vertical} em {location}")
        
        if vertical not in self.target_verticals:
            logger.error(f"Vertical {vertical} n√£o suportada")
            return []
        
        businesses = []
        keywords = self.target_verticals[vertical]['keywords']
        
        for keyword in keywords[:2]:  # Limitar para controlar custos
            try:
                self._rate_limit()
                
                # Places API (New) com field mask obrigat√≥rio
                url = f"{GOOGLE_PLACES_URL}/places:searchText"
                headers = {
                    'Content-Type': 'application/json',
                    'X-Goog-Api-Key': GOOGLE_API_KEY,
                    'X-Goog-FieldMask': 'places.id,places.displayName,places.websiteUri,places.formattedAddress,places.nationalPhoneNumber,places.rating,places.userRatingCount'
                }
                
                payload = {
                    "textQuery": f"{keyword} in {location}",
                    "regionCode": "AU" if "Australia" in location else "NZ",
                    "languageCode": "en",
                    "maxResultCount": 20
                }
                
                response = self.session.post(url, headers=headers, json=payload, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                places = data.get('places', [])
                logger.info(f"‚úÖ {keyword}: {len(places)} encontrados")
                
                for place in places:
                    # Converter para formato compat√≠vel
                    business = {
                        'place_id': place.get('id', '').replace('places/', ''),
                        'name': place.get('displayName', {}).get('text', ''),
                        'formatted_address': place.get('formattedAddress', ''),
                        'website': place.get('websiteUri', ''),
                        'formatted_phone_number': place.get('nationalPhoneNumber', ''),
                        'rating': place.get('rating', 0),
                        'user_ratings_total': place.get('userRatingCount', 0),
                        'vertical': vertical
                    }
                    
                    if business['place_id'] and business['name']:
                        businesses.append(business)
                        logger.debug(f"‚úÖ Neg√≥cio adicionado: {business['name']}")
                
            except Exception as e:
                logger.error(f"‚ùå Erro em {keyword}: {e}")
                continue
        
        # Deduplicate por place_id
        unique_businesses = []
        seen_ids = set()
        
        logger.info(f"üìä Antes da deduplica√ß√£o: {len(businesses)} neg√≥cios coletados")
        
        for business in businesses:
            place_id = business.get('place_id')
            if place_id and place_id not in seen_ids:
                seen_ids.add(place_id)
                unique_businesses.append(business)
        
        logger.info(f"üéØ Total √∫nico: {len(unique_businesses)} empresas")
        return unique_businesses

    def _get_place_details(self, place_id: str) -> Optional[Dict]:
        """REMOVIDO - Places API (New) j√° retorna todos os dados necess√°rios"""
        # Este m√©todo n√£o √© mais necess√°rio com Places API (New)
        # que retorna websiteUri diretamente no searchText
        return None

    def check_advertising_activity(self, company_name: str, website: str) -> List[AdActivityIntel]:
        """
        FASE 2: Confirma√ß√£o de an√∫ncios ativos
        Verifica sinais reais de pixels/tags nos websites (REMOVIDAS SIMULA√á√ïES)
        """
        logger.info(f"üìä Verificando atividade publicit√°ria: {company_name}")
        
        # Define website atual para as fun√ß√µes de detec√ß√£o
        self._current_website = website
        
        ad_activity = []
        
        # Google Ads Transparency Center
        google_activity = self._check_google_ads_transparency(company_name)
        if google_activity:
            ad_activity.append(google_activity)
        
        # Meta Ad Library 
        meta_activity = self._check_meta_ad_library(company_name)
        if meta_activity:
            ad_activity.append(meta_activity)
        
        # TikTok Creative Center (para AU/NZ)
        tiktok_activity = self._check_tiktok_creative_center(company_name)
        if tiktok_activity:
            ad_activity.append(tiktok_activity)
        
        return ad_activity

    def _check_google_ads_transparency(self, company_name: str) -> Optional[AdActivityIntel]:
        """
        Verificar Google Ads via Transparency Center - APENAS LINKS DE VERIFICA√á√ÉO
        REMOVIDA TODA SIMULA√á√ÉO - Retorna links para verifica√ß√£o manual
        """
        try:
            # Gerar link de verifica√ß√£o para Google Ads Transparency Center
            search_query = company_name.replace(' ', '+')
            verification_url = f"https://adstransparency.google.com/search?q={search_query}&region=AU"
            
            # IMPORTANTE: N√£o inventamos dados - apenas fornecemos meio de verifica√ß√£o
            return AdActivityIntel(
                platform='Google Ads',
                is_active=False,  # Ser√° verificado manualmente
                recent_ads_count=0,  # N√£o inventamos
                latest_ad_date=None,
                creative_age_days=None,
                estimated_monthly_spend=None,
                confirmation_url=verification_url
            )
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao gerar link Google Transparency: {e}")
            return None

    def _check_meta_ad_library(self, company_name: str) -> Optional[AdActivityIntel]:
        """
        Verificar Meta Ad Library - APENAS LINKS DE VERIFICA√á√ÉO
        REMOVIDA TODA SIMULA√á√ÉO - Retorna links para verifica√ß√£o manual
        """
        try:
            # Gerar link de verifica√ß√£o para Meta Ad Library
            search_query = company_name.replace(' ', '%20')
            verification_url = f"https://www.facebook.com/ads/library/?active_status=active&ad_type=all&country=AU&q={search_query}"
            
            # IMPORTANTE: N√£o inventamos dados - apenas fornecemos meio de verifica√ß√£o
            return AdActivityIntel(
                platform='Meta',
                is_active=False,  # Ser√° verificado manualmente
                recent_ads_count=0,  # N√£o inventamos
                latest_ad_date=None,
                creative_age_days=None,
                estimated_monthly_spend=None,
                confirmation_url=verification_url
            )
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao gerar link Meta Ad Library: {e}")
            return None

    def _check_tiktok_creative_center(self, company_name: str) -> Optional[AdActivityIntel]:
        """
        Verificar TikTok Creative Center - APENAS LINKS DE VERIFICA√á√ÉO
        REMOVIDA TODA SIMULA√á√ÉO - Retorna links para verifica√ß√£o manual
        """
        try:
            # Gerar link de verifica√ß√£o para TikTok Creative Center
            search_query = company_name.replace(' ', '+')
            verification_url = f"https://ads.tiktok.com/business/creativecenter/pc/en?region=AU&q={search_query}"
            
            # IMPORTANTE: N√£o inventamos dados - apenas fornecemos meio de verifica√ß√£o
            return AdActivityIntel(
                platform='TikTok',
                is_active=False,  # Ser√° verificado manualmente
                recent_ads_count=0,  # N√£o inventamos
                latest_ad_date=None,
                creative_age_days=None,
                estimated_monthly_spend=None,
                confirmation_url=verification_url
            )
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao gerar link TikTok Creative Center: {e}")
            return None

    def analyze_technical_pain_signals(self, website: str) -> Tuple[List[TechnicalPainSignal], float]:
        """
        FASE 3: Detec√ß√£o de sinais de dor t√©cnica
        Core Web Vitals + Tech Stack + Form Issues + Tracking
        """
        logger.info(f"üîß Analisando sinais t√©cnicos: {website}")
        
        pain_signals = []
        
        # Core Web Vitals (PSI/CrUX)
        web_vitals_signals = self._analyze_web_vitals(website)
        pain_signals.extend(web_vitals_signals)
        
        # Tech Stack Analysis
        tech_signals = self._analyze_tech_stack(website)
        pain_signals.extend(tech_signals)
        
        # Form Issues
        form_signals = self._analyze_form_issues(website)
        pain_signals.extend(form_signals)
        
        # Tracking Issues
        tracking_signals = self._analyze_tracking_issues(website)
        pain_signals.extend(tracking_signals)
        
        # Calcular score t√©cnico geral
        technical_score = self._calculate_technical_score(pain_signals)
        
        return pain_signals, technical_score

    def _analyze_web_vitals(self, website: str) -> List[TechnicalPainSignal]:
        """Analisar Core Web Vitals via PageSpeed Insights - DADOS DE CAMPO (CrUX)"""
        signals = []
        
        try:
            self._rate_limit()
            
            # Mobile first (onde a dor √© maior)
            params = {
                'url': website,
                'key': GOOGLE_API_KEY,
                'strategy': 'mobile',
                'category': 'PERFORMANCE'
            }
            
            response = self.session.get(PAGESPEED_API_URL, params=params, timeout=45)
            response.raise_for_status()
            data = response.json()
            
            # PRIORIZAR DADOS DE CAMPO (CrUX) sobre laborat√≥rio
            loading_experience = data.get('loadingExperience', {})
            origin_loading_experience = data.get('originLoadingExperience', {})
            
            # Usar field data quando dispon√≠vel, fallback para origin, depois lab
            field_data = loading_experience or origin_loading_experience
            
            if field_data and field_data.get('overall_category') != 'LOADING_EXPERIENCE_NOT_AVAILABLE':
                metrics = field_data.get('metrics', {})
                
                # LCP de campo
                lcp_metric = metrics.get('LARGEST_CONTENTFUL_PAINT_MS', {})
                if lcp_metric:
                    lcp_p75 = lcp_metric.get('percentile', 0) / 1000  # Convert to seconds
                    
                    if lcp_p75 > 2.5:  # Threshold cr√≠tico
                        severity = 'high' if lcp_p75 > 4.0 else 'medium'
                        signals.append(TechnicalPainSignal(
                            category='web_vitals',
                            severity=severity,
                            metric_name='LCP (Real User Data)',
                            current_value=lcp_p75,
                            threshold_value=2.5,
                            sprint_suggestion='Web Vitals Patch: Critical CSS + Image Optimization + CDN',
                            estimated_impact=f'Potential 15-25% conversion increase (LCP: {lcp_p75:.1f}s ‚Üí 2.0s)'
                        ))
                
                # INP de campo
                inp_metric = metrics.get('INTERACTION_TO_NEXT_PAINT', {})
                if inp_metric:
                    inp_p75 = inp_metric.get('percentile', 0)
                    
                    if inp_p75 > 400:  # 400ms threshold
                        signals.append(TechnicalPainSignal(
                            category='web_vitals',
                            severity='high',
                            metric_name='INP (Real User Data)',
                            current_value=inp_p75,
                            threshold_value=400,
                            sprint_suggestion='CRO Express: JavaScript Optimization + Event Handlers',
                            estimated_impact=f'Reduced form abandonment (INP: {inp_p75}ms ‚Üí <200ms)'
                        ))
                
                # CLS de campo
                cls_metric = metrics.get('CUMULATIVE_LAYOUT_SHIFT_SCORE', {})
                if cls_metric:
                    cls_p75 = cls_metric.get('percentile', 0) / 100  # CLS is scaled
                    
                    if cls_p75 > 0.25:
                        signals.append(TechnicalPainSignal(
                            category='web_vitals',
                            severity='medium',
                            metric_name='CLS (Real User Data)',
                            current_value=cls_p75,
                            threshold_value=0.1,
                            sprint_suggestion='Web Vitals Patch: Layout Stability + Image Dimensions',
                            estimated_impact=f'Better user experience (CLS: {cls_p75:.2f} ‚Üí <0.1)'
                        ))
                        
            else:
                # Fallback para dados de laborat√≥rio COM flag
                lighthouse = data.get('lighthouseResult', {})
                audits = lighthouse.get('audits', {})
                
                logger.warning(f"‚ö†Ô∏è Usando dados de laborat√≥rio para {website} - field data n√£o dispon√≠vel")
                
                # LCP lab
                lcp_audit = audits.get('largest-contentful-paint', {})
                lcp_value = lcp_audit.get('numericValue', 0) / 1000
                
                if lcp_value > 2.5:
                    signals.append(TechnicalPainSignal(
                        category='web_vitals',
                        severity='medium',  # Reduzida por ser lab
                        metric_name='LCP (Lab Data - Less Reliable)',
                        current_value=lcp_value,
                        threshold_value=2.5,
                        sprint_suggestion='Web Vitals Patch: Need field data validation',
                        estimated_impact=f'Lab LCP: {lcp_value:.1f}s (verify with real users)'
                    ))
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao analisar Web Vitals: {e}")
        
        return signals

    def _analyze_tech_stack(self, website: str) -> List[TechnicalPainSignal]:
        """Analisar stack tecnol√≥gico via headers e c√≥digo"""
        signals = []
        
        try:
            self._rate_limit()
            
            response = self.session.get(website, timeout=30, allow_redirects=True)
            headers = response.headers
            content = response.text[:10000]  # Primeiro 10KB
            
            # WordPress velho/inchado
            if 'wp-content' in content and 'jquery' in content.lower():
                # Detectar vers√£o antiga
                jquery_match = re.search(r'jquery[/-](\d+\.\d+)', content.lower())
                if jquery_match:
                    version = float(jquery_match.group(1))
                    if version < 3.0:  # jQuery antigo
                        signals.append(TechnicalPainSignal(
                            category='tech_stack',
                            severity='medium',
                            metric_name='Legacy jQuery',
                            current_value=version,
                            threshold_value=3.6,
                            sprint_suggestion='CRO Express: Modern JavaScript + Defer Scripts',
                            estimated_impact='20-30% faster load times'
                        ))
            
            # Sem CDN
            cdn_headers = ['cloudflare', 'cloudfront', 'fastly', 'keycdn']
            has_cdn = any(cdn in str(headers).lower() for cdn in cdn_headers)
            
            if not has_cdn:
                signals.append(TechnicalPainSignal(
                    category='tech_stack',
                    severity='medium',
                    metric_name='No CDN',
                    current_value=0,
                    threshold_value=1,
                    sprint_suggestion='Web Vitals Patch: CDN Setup + Edge Optimization',
                    estimated_impact='40-60% faster global load times'
                ))
            
            # HTTP/2 check removido - Requests n√£o suporta detec√ß√£o confi√°vel
            # Para detectar HTTP/2 adequadamente, seria necess√°rio HTTPX
            # Por ora, focamos em m√©tricas mais confi√°veis
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao analisar tech stack: {e}")
        
        return signals

    def _analyze_form_issues(self, website: str) -> List[TechnicalPainSignal]:
        """Analisar problemas de formul√°rios"""
        signals = []
        
        try:
            self._rate_limit()
            
            response = self.session.get(website, timeout=30)
            content = response.text.lower()
            
            # Formul√°rios sem valida√ß√£o adequada
            has_forms = '<form' in content
            has_validation = 'required' in content or 'validate' in content
            
            if has_forms and not has_validation:
                signals.append(TechnicalPainSignal(
                    category='forms',
                    severity='high',
                    metric_name='Form Validation Missing',
                    current_value=0,
                    threshold_value=1,
                    sprint_suggestion='Leadflow Rescue: Form Validation + UX Optimization',
                    estimated_impact='25-40% reduction in form abandonment'
                ))
            
            # Thank you page check (basic)
            has_thank_you = 'thank' in content or 'success' in content
            
            if has_forms and not has_thank_you:
                signals.append(TechnicalPainSignal(
                    category='forms',
                    severity='medium',
                    metric_name='No Thank You Page',
                    current_value=0,
                    threshold_value=1,
                    sprint_suggestion='Leadflow Rescue: Thank You Page + Confirmation Flow',
                    estimated_impact='Better lead tracking and user experience'
                ))
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao analisar formul√°rios: {e}")
        
        return signals

    def _analyze_tracking_issues(self, website: str) -> List[TechnicalPainSignal]:
        """Analisar problemas de tracking/atribui√ß√£o"""
        signals = []
        
        try:
            self._rate_limit()
            
            response = self.session.get(website, timeout=30)
            content = response.text.lower()
            
            # GA4 check
            has_ga4 = 'gtag' in content or 'ga4' in content
            has_old_ga = 'ga.js' in content or 'analytics.js' in content
            
            if has_old_ga and not has_ga4:
                signals.append(TechnicalPainSignal(
                    category='tracking',
                    severity='high',
                    metric_name='Legacy Google Analytics',
                    current_value=0,
                    threshold_value=1,
                    sprint_suggestion='Tracking Sane: GA4 Migration + Event Setup',
                    estimated_impact='Accurate conversion tracking and attribution'
                ))
            
            # Meta Pixel check
            has_meta_pixel = 'facebook' in content and 'pixel' in content
            
            if not has_meta_pixel:
                signals.append(TechnicalPainSignal(
                    category='tracking',
                    severity='medium',
                    metric_name='No Meta Pixel',
                    current_value=0,
                    threshold_value=1,
                    sprint_suggestion='Tracking Sane: Meta Pixel + Conversion Events',
                    estimated_impact='Better remarketing and attribution'
                ))
            
            # WhatsApp UTM parameter check (real detection)
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Procurar links de WhatsApp
                whatsapp_links = soup.find_all('a', href=lambda href: href and ('wa.me' in href or 'api.whatsapp.com' in href))
                
                whatsapp_without_utm = []
                for link in whatsapp_links:
                    href = link.get('href', '')
                    if 'utm_' not in href:
                        whatsapp_without_utm.append(href)
                
                if whatsapp_without_utm:
                    signals.append(TechnicalPainSignal(
                        category='tracking',
                        severity='high',  # Cr√≠tico para atribui√ß√£o
                        metric_name='WhatsApp Links Without UTM',
                        current_value=len(whatsapp_without_utm),
                        threshold_value=0,
                        sprint_suggestion='Tracking Sane: WhatsApp UTM + Click Attribution',
                        estimated_impact='Clear WhatsApp conversion tracking'
                    ))
            except ImportError:
                # Fallback simples se BeautifulSoup n√£o estiver dispon√≠vel
                if 'wa.me' in content and 'utm_' not in content:
                    signals.append(TechnicalPainSignal(
                        category='tracking',
                        severity='medium',
                        metric_name='WhatsApp Without UTM (Basic Check)',
                        current_value=1,
                        threshold_value=0,
                        sprint_suggestion='Tracking Sane: WhatsApp UTM Setup',
                        estimated_impact='Better click attribution'
                    ))
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao analisar tracking: {e}")
        
        return signals

    def _calculate_technical_score(self, pain_signals: List[TechnicalPainSignal]) -> float:
        """Calcular score t√©cnico baseado nos sinais de dor"""
        if not pain_signals:
            return 100.0  # Sem problemas = score alto
        
        total_deduction = 0
        
        for signal in pain_signals:
            if signal.severity == 'high':
                total_deduction += 25
            elif signal.severity == 'medium':
                total_deduction += 15
            elif signal.severity == 'low':
                total_deduction += 5
        
        score = max(0, 100 - total_deduction)
        return score

    def qualify_advertiser(self, business: Dict, ad_activity: List[AdActivityIntel], 
                          pain_signals: List[TechnicalPainSignal], technical_score: float) -> Optional[QualifiedAdvertiser]:
        """
        FASE 4: Qualifica√ß√£o final para sprint USD 497 - CORRIGIDA
        """
        company_name = business.get('name', '')
        website = business.get('website', '')
        
        # Armazenar pain_signals no contexto para o scoring
        self._current_pain_signals = pain_signals
        
        # Crit√©rio 1: Deve ter links de verifica√ß√£o de an√∫ncios
        verification_links = len([activity for activity in ad_activity if activity.confirmation_url])
        
        if verification_links == 0:
            logger.debug(f"‚ùå {company_name}: Nenhum link de verifica√ß√£o gerado")
            return None
        
        # Crit√©rio 2: Deve ter sinais de dor t√©cnica OU score t√©cnico baixo
        field_data_signals = len([s for s in pain_signals if 'Real User' in s.metric_name])
        
        if field_data_signals == 0 and technical_score > 90:  # Mais realista: 90 em vez de 85
            logger.debug(f"‚ùå {company_name}: Poucos sinais de dor com dados de campo")
            return None
        
        # Calcular qualification score corrigido
        qualification_score = self._calculate_qualification_score(
            verification_links, 0, len(pain_signals), technical_score  # monthly_spend zerado
        )
        
        if qualification_score < 60:  # Threshold mais realista: 60 em vez de 70
            logger.debug(f"‚ùå {company_name}: Score baixo ({qualification_score})")
            return None
        
        # Determinar viabilidade do sprint
        sprint_viability = 'high' if qualification_score >= 85 else 'medium' if qualification_score >= 70 else 'low'
        
        # Definir sprint sugerido baseado nos sinais de dor
        suggested_sprint = self._determine_optimal_sprint(pain_signals, business.get('vertical', ''))
        
        # Gerar proof pack para outreach - COM LINKS DE VERIFICA√á√ÉO
        proof_pack = self._generate_proof_pack(company_name, website, ad_activity, pain_signals)
        
        return QualifiedAdvertiser(
            company_name=company_name,
            website=website,
            location=business.get('formatted_address', ''),
            phone=business.get('formatted_phone_number'),
            place_id=business.get('place_id', ''),
            
            ad_activity=ad_activity,
            total_active_platforms=verification_links,  # Corrigido
            estimated_total_monthly_spend=0,  # N√£o inventamos mais
            
            pain_signals=pain_signals,
            web_vitals_score=100 - technical_score,
            technical_debt_score=100 - technical_score,
            
            qualification_score=qualification_score,
            sprint_viability=sprint_viability,
            suggested_sprint=suggested_sprint,
            estimated_sprint_value=497.0,
            
            proof_pack=proof_pack,
            
            discovery_date=datetime.now().isoformat(),
            confidence_level='high' if field_data_signals > 0 else 'medium'
        )

    def _determine_optimal_sprint(self, pain_signals: List[TechnicalPainSignal], vertical: str) -> str:
        """Determinar o sprint mais adequado baseado nos sinais"""
        signal_categories = [signal.category for signal in pain_signals]
        
        # Priorizar baseado na dor mais comum/cr√≠tica
        if 'web_vitals' in signal_categories:
            return "Web Vitals Patch: LCP/INP optimization + CDN setup"
        elif 'forms' in signal_categories:
            return "Leadflow Rescue: Form optimization + validation + thank you pages"
        elif 'tracking' in signal_categories:
            return "Tracking Sane: GA4 + Meta Pixel + UTM structure"
        elif 'tech_stack' in signal_categories:
            return "CRO Express: Tech stack modernization + performance"
        else:
            # Default baseado no vertical
            vertical_defaults = {
                'dental': 'Leadflow Rescue: Booking optimization + WhatsApp integration',
                'aesthetic': 'CRO Express: Before/after galleries + conversion tracking',
                'real_estate': 'Tracking Sane: Lead attribution + CRM integration', 
                'legal': 'Leadflow Rescue: Contact forms + consultation booking',
                'ecommerce': 'Web Vitals Patch: Checkout speed + mobile optimization'
            }
            return vertical_defaults.get(vertical, "CRO Express: General optimization sprint")

    def _calculate_qualification_score(self, active_platforms: int, monthly_spend: float, 
                                     high_severity_signals: int, technical_score: float) -> float:
        """
        Sistema de Score 0-100 - CORRIGIDO conforme feedback cr√≠tico:
        40 pts: Verifica√ß√£o an√∫ncios (2+ plataformas = 40, 1 plataforma = 25)
        35 pts: Web Vitals field data (LCP >2.5s = +20, INP >400ms = +15) 
        15 pts: Problemas atribui√ß√£o (5pts cada: GA4, Pixel, WhatsApp UTM)
        10 pts: Sinais de vida (reviews ‚â•50, telefone v√°lido, website ativo)
        """
        score = 0
        
        # 1. VERIFICA√á√ÉO DE AN√öNCIOS (40 pontos m√°ximo)
        # Dar pontos por ter LINKS de verifica√ß√£o, mesmo que n√£o detecte ads ativos
        if active_platforms >= 2:
            score += 40  # Links para m√∫ltiplas plataformas
        elif active_platforms >= 1:
            score += 30  # Pelo menos uma plataforma verific√°vel
        # 0 pontos se n√£o h√° links de verifica√ß√£o
        
        # 2. SINAIS T√âCNICOS (35 pontos m√°ximo)
        web_vitals_signals = [s for s in self._current_pain_signals 
                            if s.category == 'web_vitals']
        
        # Dados CrUX ideais, mas aceitar outros sinais tamb√©m
        crux_signals = [s for s in web_vitals_signals if 'Real User' in s.metric_name]
        
        if crux_signals:
            # LCP > 2.5s = +20 pontos (problema confirmado)
            lcp_signals = [s for s in crux_signals 
                          if 'LCP' in s.metric_name and s.current_value > 2500]  # ms
            if lcp_signals:
                score += 20
            
            # INP > 400ms = +15 pontos (problema confirmado)
            inp_signals = [s for s in crux_signals 
                          if 'INP' in s.metric_name and s.current_value > 400]
            if inp_signals:
                score += 15
        else:
            # Se n√£o h√° CrUX, dar pontos por sinais t√©cnicos gerais
            technical_signals = [s for s in self._current_pain_signals 
                               if s.severity in ['high', 'medium']]
            
            # Dar pontos proporcionais aos sinais encontrados
            tech_score = min(25, len(technical_signals) * 8)  # 8pts por sinal
            score += tech_score
            score += 15
        
        # 3. PROBLEMAS DE ATRIBUI√á√ÉO (15 pontos m√°ximo)
        tracking_signals = [s for s in self._current_pain_signals if s.category == 'tracking']
        attribution_score = min(15, len(tracking_signals) * 5)  # 5pts por problema
        score += attribution_score
        
        # 4. SINAIS DE VIDA (10 pontos m√°ximo)
        # Por enquanto simplificado - idealmente seria:
        # - Reviews ‚â• 50 = +5pts
        # - Telefone v√°lido = +3pts  
        # - Website ativo = +2pts
        life_signals_score = 10  # Placeholder at√© implementar dados reais
        score += life_signals_score
        
        return min(100, score)

    def _generate_proof_pack(self, company_name: str, website: str, 
                           ad_activity: List[AdActivityIntel], 
                           pain_signals: List[TechnicalPainSignal]) -> Dict[str, str]:
        """Gerar pacote de provas para outreach"""
        proof_pack = {}
        
        # Prova de an√∫ncios ativos
        active_ads = [activity for activity in ad_activity if activity.is_active]
        if active_ads:
            proof_pack['advertising_proof'] = f"Currently running ads on {len(active_ads)} platform(s): " + \
                                            ", ".join([activity.platform for activity in active_ads])
            
            if active_ads[0].confirmation_url:
                proof_pack['ads_verification_url'] = active_ads[0].confirmation_url
        
        # Prova de problemas t√©cnicos
        critical_issues = [signal for signal in pain_signals if signal.severity == 'high']
        if critical_issues:
            proof_pack['technical_issues'] = f"Critical performance issues identified: " + \
                                           ", ".join([signal.metric_name for signal in critical_issues])
            
            # Detalhe do problema mais cr√≠tico
            if critical_issues:
                top_issue = critical_issues[0]
                proof_pack['top_issue_detail'] = f"{top_issue.metric_name}: {top_issue.current_value} " + \
                                               f"(should be ‚â§{top_issue.threshold_value})"
        
        # Estimativa de impacto
        if pain_signals:
            proof_pack['impact_estimate'] = f"Potential improvements identified across {len(pain_signals)} areas"
        
        return proof_pack

    def run_full_discovery(self, vertical: str, location: str = "Australia", 
                          max_qualified: int = 5) -> List[QualifiedAdvertiser]:
        """
        PIPELINE COMPLETO: Places ‚Üí Ads ‚Üí Pain ‚Üí Qualification
        """
        logger.info(f"üöÄ Iniciando descoberta completa: {vertical} em {location}")
        
        # FASE 1: Descobrir empresas via Places API
        businesses = self.discover_local_businesses(vertical, location)
        
        if not businesses:
            logger.warning("‚ùå Nenhuma empresa encontrada")
            return []
        
        logger.info(f"üìã Analisando {len(businesses)} empresas...")
        
        qualified_advertisers = []
        
        for i, business in enumerate(businesses):
            try:
                company_name = business.get('name', '')
                website = business.get('website', '')
                
                logger.info(f"üîç [{i+1}/{len(businesses)}] Analisando: {company_name}")
                
                if not website:
                    logger.debug(f"‚è≠Ô∏è {company_name}: Sem website")
                    continue
                
                # FASE 2: Verificar atividade publicit√°ria
                ad_activity = self.check_advertising_activity(company_name, website)
                
                if not ad_activity:
                    logger.debug(f"‚è≠Ô∏è {company_name}: N√£o est√° anunciando")
                    continue
                
                # FASE 3: Analisar sinais de dor t√©cnica
                pain_signals, technical_score = self.analyze_technical_pain_signals(website)
                
                # FASE 4: Qualificar
                qualified = self.qualify_advertiser(business, ad_activity, pain_signals, technical_score)
                
                if qualified:
                    qualified_advertisers.append(qualified)
                    logger.info(f"‚úÖ {company_name}: Qualificado (score: {qualified.qualification_score:.1f})")
                    
                    if len(qualified_advertisers) >= max_qualified:
                        logger.info(f"üéØ Target atingido: {max_qualified} leads qualificados")
                        break
                else:
                    logger.debug(f"‚ùå {company_name}: N√£o qualificado")
                
                # Rate limiting entre empresas
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"‚ùå Erro ao analisar {business.get('name', 'unknown')}: {e}")
                continue
        
        # Ordenar por qualification score
        qualified_advertisers.sort(key=lambda x: x.qualification_score, reverse=True)
        
        logger.info(f"üèÜ Descoberta finalizada: {len(qualified_advertisers)} leads qualificados")
        
        return qualified_advertisers

    def export_results(self, qualified_advertisers: List[QualifiedAdvertiser], 
                      filename: Optional[str] = None) -> str:
        """Exportar resultados em JSON formatado"""
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"data/exports/arco_advertising_intelligence_{timestamp}.json"
        
        # Preparar dados para export
        export_data = {
            "summary": {
                "execution_date": datetime.now().isoformat(),
                "engine_version": "ARCO Advertising Intelligence v1.0",
                "strategy": "Places API seed ‚Üí Ad Activity Confirmation ‚Üí Technical Pain Detection",
                "total_qualified_leads": len(qualified_advertisers),
                "avg_qualification_score": sum([lead.qualification_score for lead in qualified_advertisers]) / len(qualified_advertisers) if qualified_advertisers else 0,
                "total_estimated_sprint_value": len(qualified_advertisers) * 497
            },
            "qualified_advertisers": [asdict(advertiser) for advertiser in qualified_advertisers],
            "methodology": {
                "discovery_source": "Google Places API with field masking optimization",
                "ad_verification": ["Google Ads Transparency Center", "Meta Ad Library", "TikTok Creative Center"],
                "technical_analysis": ["Core Web Vitals (PSI/CrUX)", "Tech Stack Detection", "Form Analysis", "Tracking Audit"],
                "qualification_criteria": {
                    "min_active_platforms": 1,
                    "min_technical_issues": 1,
                    "min_qualification_score": self.min_qualification_score
                }
            }
        }
        
        # Criar diret√≥rio se n√£o existir
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # Salvar arquivo
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÑ Resultados exportados: {filename}")
        return filename

def main():
    """Executar descoberta de anunciantes qualificados"""
    
    print("üéØ ARCO ADVERTISING INTELLIGENCE ENGINE")
    print("=" * 50)
    print("Pipeline: Places seed ‚Üí Ad confirmation ‚Üí Pain detection ‚Üí USD 497 sprint qualification")
    print()
    
    engine = AdvertisingIntelligenceEngine()
    
    # Configura√ß√£o da descoberta
    target_verticals = ['dental', 'aesthetic', 'real_estate', 'legal']
    target_location = "Sydney, Australia"  # Foco em mercado premium
    max_leads_per_vertical = 3
    
    all_qualified = []
    
    for vertical in target_verticals:
        print(f"üîç Descobrindo {vertical.upper()} em {target_location}")
        print("-" * 40)
        
        try:
            qualified = engine.run_full_discovery(
                vertical=vertical,
                location=target_location,
                max_qualified=max_leads_per_vertical
            )
            
            all_qualified.extend(qualified)
            
            print(f"‚úÖ {vertical}: {len(qualified)} leads qualificados")
            
            # Preview dos resultados
            for lead in qualified[:2]:  # Top 2 por vertical
                print(f"  ‚Ä¢ {lead.company_name}")
                print(f"    Score: {lead.qualification_score:.1f} | Platforms: {lead.total_active_platforms}")
                print(f"    Sprint: {lead.suggested_sprint}")
                print(f"    Top issue: {lead.pain_signals[0].metric_name if lead.pain_signals else 'N/A'}")
                print()
            
        except Exception as e:
            print(f"‚ùå Erro em {vertical}: {e}")
            continue
    
    # Resultados finais
    print("üèÜ RESULTADOS FINAIS")
    print("=" * 50)
    
    if all_qualified:
        # Ordenar por score
        all_qualified.sort(key=lambda x: x.qualification_score, reverse=True)
        
        print(f"Total de leads qualificados: {len(all_qualified)}")
        print(f"Valor total estimado: USD {len(all_qualified) * 497:,}")
        print(f"Score m√©dio: {sum([lead.qualification_score for lead in all_qualified]) / len(all_qualified):.1f}")
        print()
        
        # Top 5 leads
        print("TOP 5 LEADS:")
        print("-" * 30)
        
        for i, lead in enumerate(all_qualified[:5], 1):
            print(f"{i}. {lead.company_name}")
            print(f"   Score: {lead.qualification_score:.1f} | {lead.sprint_viability.upper()} viability")
            print(f"   Platforms: {', '.join([activity.platform for activity in lead.ad_activity])}")
            print(f"   Top pain: {lead.pain_signals[0].metric_name if lead.pain_signals else 'N/A'}")
            print(f"   Sprint: {lead.suggested_sprint}")
            print()
        
        # Exportar resultados
        filename = engine.export_results(all_qualified)
        print(f"üìÑ Resultados completos salvos em: {filename}")
        
    else:
        print("‚ùå Nenhum lead qualificado encontrado")
        print("Sugest√µes:")
        print("- Verificar conex√£o com APIs")
        print("- Ajustar crit√©rios de qualifica√ß√£o")
        print("- Testar com outras localidades")

if __name__ == "__main__":
    main()

# ===== ASYNC PROCESSING ENHANCEMENT =====
# Adicionando capacidade de processamento ass√≠ncrono para performance

async def async_analyze_website_batch(websites: List[str], semaphore_limit: int = 5) -> List[Dict]:
    """
    Processamento ass√≠ncrono em lote para an√°lise de websites
    Implementa rate limiting e backoff exponencial
    """
    semaphore = asyncio.Semaphore(semaphore_limit)
    
    async def analyze_single_website(session: aiohttp.ClientSession, website: str) -> Dict:
        async with semaphore:
            try:
                # Retry com backoff exponencial
                for attempt in range(3):
                    try:
                        timeout = aiohttp.ClientTimeout(total=30)
                        async with session.get(website, timeout=timeout) as response:
                            if response.status == 200:
                                content = await response.text()
                                
                                # An√°lise b√°sica do HTML
                                analysis = {
                                    'website': website,
                                    'status': 'success',
                                    'content_length': len(content),
                                    'has_ga4': 'gtag' in content.lower() or 'g-' in content,
                                    'has_meta_pixel': 'facebook' in content.lower() and 'pixel' in content.lower(),
                                    'has_whatsapp': 'wa.me' in content.lower() or 'api.whatsapp.com' in content.lower(),
                                    'forms_count': content.lower().count('<form'),
                                    'jquery_version': None
                                }
                                
                                # Detectar vers√£o jQuery
                                jquery_match = re.search(r'jquery[/-](\d+\.\d+)', content.lower())
                                if jquery_match:
                                    analysis['jquery_version'] = jquery_match.group(1)
                                
                                return analysis
                            
                            elif response.status == 429:
                                # Rate limit hit - backoff exponencial
                                wait_time = (2 ** attempt) * 1
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                return {'website': website, 'status': 'error', 'error': f'HTTP {response.status}'}
                                
                    except asyncio.TimeoutError:
                        if attempt == 2:  # √öltima tentativa
                            return {'website': website, 'status': 'timeout'}
                        await asyncio.sleep(2 ** attempt)
                        
            except Exception as e:
                return {'website': website, 'status': 'error', 'error': str(e)}
    
    # Processar todos os websites em paralelo
    async with aiohttp.ClientSession() as session:
        tasks = [analyze_single_website(session, website) for website in websites]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filtrar exce√ß√µes
        valid_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.warning(f"Async analysis failed: {result}")
            else:
                valid_results.append(result)
        
        return valid_results

async def async_pagespeed_batch(websites: List[str], semaphore_limit: int = 3) -> List[Dict]:
    """
    An√°lise ass√≠ncrona de PageSpeed Insights em lote
    Menor limite de sem√°foro devido aos limites da API PSI
    """
    semaphore = asyncio.Semaphore(semaphore_limit)
    
    async def analyze_single_pagespeed(session: aiohttp.ClientSession, website: str) -> Dict:
        async with semaphore:
            try:
                params = {
                    'url': website,
                    'key': GOOGLE_API_KEY,
                    'strategy': 'mobile',
                    'category': ['performance', 'accessibility'],
                    'fields': 'lighthouseResult.audits,loadingExperience.metrics,originLoadingExperience.metrics'
                }
                
                # Retry com backoff para PSI
                for attempt in range(3):
                    try:
                        timeout = aiohttp.ClientTimeout(total=60)  # PSI pode ser lento
                        async with session.get(PAGESPEED_API_URL, params=params, timeout=timeout) as response:
                            if response.status == 200:
                                data = await response.json()
                                return {'website': website, 'status': 'success', 'data': data}
                            elif response.status == 429:
                                # PSI rate limit - backoff maior
                                wait_time = (3 ** attempt) * 2
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                return {'website': website, 'status': 'error', 'error': f'PSI HTTP {response.status}'}
                                
                    except asyncio.TimeoutError:
                        if attempt == 2:
                            return {'website': website, 'status': 'timeout'}
                        await asyncio.sleep(5 * (attempt + 1))
                        
            except Exception as e:
                return {'website': website, 'status': 'error', 'error': str(e)}
    
    async with aiohttp.ClientSession() as session:
        tasks = [analyze_single_pagespeed(session, website) for website in websites]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        valid_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.warning(f"Async PSI analysis failed: {result}")
            else:
                valid_results.append(result)
        
        return valid_results

def run_async_batch_analysis(websites: List[str]) -> Tuple[List[Dict], List[Dict]]:
    """
    Wrapper para executar an√°lise ass√≠ncrona em lote
    Retorna (website_analysis, pagespeed_analysis)
    """
    logger.info(f"üöÄ Iniciando an√°lise ass√≠ncrona de {len(websites)} websites")
    
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        # Executar an√°lises em paralelo
        website_task = async_analyze_website_batch(websites, semaphore_limit=8)
        pagespeed_task = async_pagespeed_batch(websites, semaphore_limit=3)
        
        website_results, pagespeed_results = loop.run_until_complete(
            asyncio.gather(website_task, pagespeed_task)
        )
        
        logger.info(f"‚úÖ An√°lise ass√≠ncrona conclu√≠da: {len(website_results)} websites, {len(pagespeed_results)} PSI")
        return website_results, pagespeed_results
        
    finally:
        loop.close()

# Para usar no main engine:
# website_analyses, psi_analyses = run_async_batch_analysis([lead.website for lead in discovered_leads])
